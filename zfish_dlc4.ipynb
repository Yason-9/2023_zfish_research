{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM3KSgjMaei3bM4zsSUmH9d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yason-9/2023_zfish_research/blob/main/zfish_dlc4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEBAaJF56zTx",
        "outputId": "c66766df-34c0-44b9-9d16-f2bbc50ab155"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting https://github.com/DeepLabCut/DeepLabCut/archive/master.zip\n",
            "  Downloading https://github.com/DeepLabCut/DeepLabCut/archive/master.zip\n",
            "\u001b[2K     \u001b[32m/\u001b[0m \u001b[32m75.5 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m \u001b[33m0:00:03\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting dlclibrary (from deeplabcut==2.3.5)\n",
            "  Downloading dlclibrary-0.0.3-py3-none-any.whl (14 kB)\n",
            "Collecting filterpy>=1.4.4 (from deeplabcut==2.3.5)\n",
            "  Downloading filterpy-1.4.5.zip (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.0/178.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ruamel.yaml>=0.15.0 (from deeplabcut==2.3.5)\n",
            "  Downloading ruamel.yaml-0.17.32-py3-none-any.whl (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: imgaug>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from deeplabcut==2.3.5) (0.4.0)\n",
            "Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.10/dist-packages (from deeplabcut==2.3.5) (0.4.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.10/dist-packages (from deeplabcut==2.3.5) (0.56.4)\n",
            "Requirement already satisfied: matplotlib>=3.3 in /usr/local/lib/python3.10/dist-packages (from deeplabcut==2.3.5) (3.7.1)\n",
            "Requirement already satisfied: networkx>=2.6 in /usr/local/lib/python3.10/dist-packages (from deeplabcut==2.3.5) (3.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from deeplabcut==2.3.5) (1.22.4)\n",
            "Requirement already satisfied: pandas!=1.5.0,>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from deeplabcut==2.3.5) (1.5.3)\n",
            "Requirement already satisfied: scikit-image>=0.17 in /usr/local/lib/python3.10/dist-packages (from deeplabcut==2.3.5) (0.19.3)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.10/dist-packages (from deeplabcut==2.3.5) (1.2.2)\n",
            "Requirement already satisfied: scipy<1.11.0,>=1.4 in /usr/local/lib/python3.10/dist-packages (from deeplabcut==2.3.5) (1.10.1)\n",
            "Requirement already satisfied: statsmodels>=0.11 in /usr/local/lib/python3.10/dist-packages (from deeplabcut==2.3.5) (0.13.5)\n",
            "Requirement already satisfied: tables>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from deeplabcut==2.3.5) (3.8.0)\n",
            "Collecting torch<=1.12 (from deeplabcut==2.3.5)\n",
            "  Downloading torch-1.12.0-cp310-cp310-manylinux1_x86_64.whl (776.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorpack>=0.11 (from deeplabcut==2.3.5)\n",
            "  Downloading tensorpack-0.11-py2.py3-none-any.whl (296 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.3/296.3 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tf_slim>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from deeplabcut==2.3.5) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deeplabcut==2.3.5) (4.65.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from deeplabcut==2.3.5) (6.0)\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.10/dist-packages (from deeplabcut==2.3.5) (8.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from imgaug>=0.4.0->deeplabcut==2.3.5) (1.16.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from imgaug>=0.4.0->deeplabcut==2.3.5) (4.7.0.72)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from imgaug>=0.4.0->deeplabcut==2.3.5) (2.25.1)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.10/dist-packages (from imgaug>=0.4.0->deeplabcut==2.3.5) (2.0.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->deeplabcut==2.3.5) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->deeplabcut==2.3.5) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->deeplabcut==2.3.5) (4.40.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->deeplabcut==2.3.5) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->deeplabcut==2.3.5) (23.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->deeplabcut==2.3.5) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->deeplabcut==2.3.5) (2.8.2)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.54->deeplabcut==2.3.5) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.54->deeplabcut==2.3.5) (67.7.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=1.5.0,>=1.0.1->deeplabcut==2.3.5) (2022.7.1)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.15.0->deeplabcut==2.3.5)\n",
            "  Downloading ruamel.yaml.clib-0.2.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (485 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.6/485.6 kB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.17->deeplabcut==2.3.5) (2023.7.4)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.17->deeplabcut==2.3.5) (1.4.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->deeplabcut==2.3.5) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->deeplabcut==2.3.5) (3.1.0)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.11->deeplabcut==2.3.5) (0.5.3)\n",
            "Requirement already satisfied: cython>=0.29.21 in /usr/local/lib/python3.10/dist-packages (from tables>=3.7.0->deeplabcut==2.3.5) (0.29.36)\n",
            "Requirement already satisfied: numexpr>=2.6.2 in /usr/local/lib/python3.10/dist-packages (from tables>=3.7.0->deeplabcut==2.3.5) (2.8.4)\n",
            "Requirement already satisfied: blosc2~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from tables>=3.7.0->deeplabcut==2.3.5) (2.0.0)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from tables>=3.7.0->deeplabcut==2.3.5) (9.0.0)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from tensorpack>=0.11->deeplabcut==2.3.5) (2.3.0)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from tensorpack>=0.11->deeplabcut==2.3.5) (0.8.10)\n",
            "Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from tensorpack>=0.11->deeplabcut==2.3.5) (1.0.5)\n",
            "Collecting msgpack-numpy>=0.4.4.2 (from tensorpack>=0.11->deeplabcut==2.3.5)\n",
            "  Downloading msgpack_numpy-0.4.8-py2.py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: pyzmq>=16 in /usr/local/lib/python3.10/dist-packages (from tensorpack>=0.11->deeplabcut==2.3.5) (23.2.1)\n",
            "Requirement already satisfied: psutil>=5 in /usr/local/lib/python3.10/dist-packages (from tensorpack>=0.11->deeplabcut==2.3.5) (5.9.5)\n",
            "Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from tf_slim>=1.1.0->deeplabcut==2.3.5) (1.4.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<=1.12->deeplabcut==2.3.5) (4.7.1)\n",
            "Collecting huggingface-hub (from dlclibrary->deeplabcut==2.3.5)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->dlclibrary->deeplabcut==2.3.5) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->dlclibrary->deeplabcut==2.3.5) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->dlclibrary->deeplabcut==2.3.5) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->dlclibrary->deeplabcut==2.3.5) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->dlclibrary->deeplabcut==2.3.5) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->dlclibrary->deeplabcut==2.3.5) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->dlclibrary->deeplabcut==2.3.5) (3.4)\n",
            "Building wheels for collected packages: deeplabcut, filterpy\n",
            "  Building wheel for deeplabcut (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deeplabcut: filename=deeplabcut-2.3.5-py3-none-any.whl size=1381932 sha256=0c827d909b065b2c15dcf3f1679295403595f565a0e8c59c0a0f75da6840380d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-iedd2jeb/wheels/ed/52/7a/acef672cccff8889bdeb5d574e5f6ae39fa05735ee9ada0c35\n",
            "  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for filterpy: filename=filterpy-1.4.5-py3-none-any.whl size=110459 sha256=39cf104ec489a0f3a557ee5a561ddc64320fe0211c5b3f055ed4cee6b55f73cc\n",
            "  Stored in directory: /root/.cache/pip/wheels/0f/0c/ea/218f266af4ad626897562199fbbcba521b8497303200186102\n",
            "Successfully built deeplabcut filterpy\n",
            "Installing collected packages: torch, ruamel.yaml.clib, msgpack-numpy, tensorpack, ruamel.yaml, huggingface-hub, filterpy, dlclibrary, deeplabcut\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.1+cu118\n",
            "    Uninstalling torch-2.0.1+cu118:\n",
            "      Successfully uninstalled torch-2.0.1+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 1.12.0 which is incompatible.\n",
            "torchdata 0.6.1 requires torch==2.0.1, but you have torch 1.12.0 which is incompatible.\n",
            "torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.12.0 which is incompatible.\n",
            "torchvision 0.15.2+cu118 requires torch==2.0.1, but you have torch 1.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed deeplabcut-2.3.5 dlclibrary-0.0.3 filterpy-1.4.5 huggingface-hub-0.16.4 msgpack-numpy-0.4.8 ruamel.yaml-0.17.32 ruamel.yaml.clib-0.2.7 tensorpack-0.11 torch-1.12.0\n",
            "Requirement already satisfied: tf_slim in /usr/local/lib/python3.10/dist-packages (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from tf_slim) (1.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install https://github.com/DeepLabCut/DeepLabCut/archive/master.zip\n",
        "!pip install tf_slim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import deeplabcut as dlc\n",
        "import tensorflow\n",
        "tensorflow.__version__\n",
        "\n",
        "import os"
      ],
      "metadata": {
        "id": "bYct03Rq8ZiS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddd60b01-605b-42a4-86b0-1812f8d3c7d7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Loading DLC 2.3.5...\n",
            "DLC loaded in light mode; you cannot use any GUI (labeling, relabeling and standalone GUI)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ProjectFolderName = 'test-dlc-2023-07-06'\n",
        "VideoType = \"avi\"\n",
        "\n",
        "\n",
        "videofile_path = ['/content/drive/My Drive/'+ProjectFolderName+'/videos/']\n",
        "\n",
        "path_config_file = '/content/drive/My Drive/'+ProjectFolderName+'/config.yaml'\n"
      ],
      "metadata": {
        "id": "UM0LPZzD84h4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SHUF = 1"
      ],
      "metadata": {
        "id": "9OBdNtNNt5Uo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dlc.create_training_dataset(path_config_file, net_type='resnet_50', Shuffles=[SHUF], augmenter_type='imgaug')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkKJy1rJ_3HW",
        "outputId": "97fa8ad2-f2c6-480a-92ce-f662d446dc13"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading a ImageNet-pretrained model from http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz....\n",
            "The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.95,\n",
              "  1,\n",
              "  (array([314, 267,  15,  65, 214,  63, 238, 161, 166, 196,  64, 103, 124,\n",
              "          249, 240,  60, 122, 270,  66,  26, 184, 157, 136, 176, 258, 198,\n",
              "          309,   7,   6, 291, 252,  22, 279,  21,  55, 156,  12, 108,  68,\n",
              "          144, 236, 181,  59, 134, 229,  92, 228, 266,  74,  81, 287, 304,\n",
              "          263,  52, 133, 175,  56,  90, 225,  17,   1,   8, 218, 253, 290,\n",
              "          140, 215, 234,   5,  33, 150,  34, 210, 213, 262, 137,  45, 101,\n",
              "          286, 126, 160, 168, 250, 226, 120, 220, 135,  73, 312, 303, 116,\n",
              "          281,  29,  97,  20,  46, 289, 261, 209,  89, 248,  27, 171, 276,\n",
              "           37,  54, 191, 259, 142, 132, 310, 189, 302, 237, 158, 153,  35,\n",
              "          102,  76, 106, 170, 223, 200, 159, 173, 318, 300,  44, 145, 129,\n",
              "          224, 111, 208, 299,  18, 194,  79,  71, 167, 268, 306, 146, 188,\n",
              "           83, 241, 118, 152, 298, 110,  16,  75, 109, 206, 190, 139,   4,\n",
              "           96, 217,  61,  67, 154, 164, 233, 179, 297,  40, 219,  13, 107,\n",
              "          231,   3, 311, 125,  24,  30,  77, 295, 221, 199,  19, 272, 274,\n",
              "          182, 283,  80,  51,   2,  11, 104, 212,  86,  10, 246,  58,  41,\n",
              "           14, 155,  50, 247, 301, 264, 313, 123, 235,  62, 222, 260, 130,\n",
              "          187, 255, 216, 245,  43, 114, 138, 230, 256, 201, 149, 112, 205,\n",
              "          269,  98, 254,  93, 239, 162,  36, 178, 113,   0,  94, 293,  95,\n",
              "          315, 278, 169,  69,  49,  48,  85, 316, 141, 207,  23, 186, 227,\n",
              "          148, 143,  78, 232, 180, 100, 204, 131, 271, 284, 317, 203,  84,\n",
              "          121, 273, 308,  91,  82, 119,  57, 257, 282,  42, 105, 280,  38,\n",
              "           53, 275, 128,  28, 183, 163, 151, 244, 202,  31,  32, 127, 185,\n",
              "          296, 288, 147, 285, 294, 177,  99, 197, 243, 115, 265,  72,  25,\n",
              "          165, 305, 174, 307]),\n",
              "   array([ 39, 193,  88,  70,  87, 292, 242, 277, 211,   9, 195, 251, 192,\n",
              "          117,  47, 172])))]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dlc.train_network(path_config_file, shuffle=SHUF, displayiters=10,saveiters=500)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "easWOG5I0jj0",
        "outputId": "ad4fcf2b-690a-4a82-bf6f-afb567cdb998"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selecting single-animal trainer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Config:\n",
            "{'all_joints': [[0], [1], [2], [3]],\n",
            " 'all_joints_names': ['left eye', 'right eye', 'lower head', 'tail'],\n",
            " 'alpha_r': 0.02,\n",
            " 'apply_prob': 0.5,\n",
            " 'batch_size': 1,\n",
            " 'contrast': {'clahe': True,\n",
            "              'claheratio': 0.1,\n",
            "              'histeq': True,\n",
            "              'histeqratio': 0.1},\n",
            " 'convolution': {'edge': False,\n",
            "                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},\n",
            "                 'embossratio': 0.1,\n",
            "                 'sharpen': False,\n",
            "                 'sharpenratio': 0.3},\n",
            " 'crop_pad': 0,\n",
            " 'cropratio': 0.4,\n",
            " 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_testJul6/test_dlc95shuffle1.mat',\n",
            " 'dataset_type': 'imgaug',\n",
            " 'decay_steps': 30000,\n",
            " 'deterministic': False,\n",
            " 'display_iters': 1000,\n",
            " 'fg_fraction': 0.25,\n",
            " 'global_scale': 0.8,\n",
            " 'init_weights': '/usr/local/lib/python3.10/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt',\n",
            " 'intermediate_supervision': False,\n",
            " 'intermediate_supervision_layer': 12,\n",
            " 'location_refinement': True,\n",
            " 'locref_huber_loss': True,\n",
            " 'locref_loss_weight': 0.05,\n",
            " 'locref_stdev': 7.2801,\n",
            " 'log_dir': 'log',\n",
            " 'lr_init': 0.0005,\n",
            " 'max_input_size': 1500,\n",
            " 'mean_pixel': [123.68, 116.779, 103.939],\n",
            " 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_testJul6/Documentation_data-test_95shuffle1.pickle',\n",
            " 'min_input_size': 64,\n",
            " 'mirror': False,\n",
            " 'multi_stage': False,\n",
            " 'multi_step': [[0.005, 10000],\n",
            "                [0.02, 430000],\n",
            "                [0.002, 730000],\n",
            "                [0.001, 1030000]],\n",
            " 'net_type': 'resnet_50',\n",
            " 'num_joints': 4,\n",
            " 'optimizer': 'sgd',\n",
            " 'pairwise_huber_loss': False,\n",
            " 'pairwise_predict': False,\n",
            " 'partaffinityfield_predict': False,\n",
            " 'pos_dist_thresh': 17,\n",
            " 'project_path': '/content/drive/My Drive/test-dlc-2023-07-06',\n",
            " 'regularize': False,\n",
            " 'rotation': 25,\n",
            " 'rotratio': 0.4,\n",
            " 'save_iters': 50000,\n",
            " 'scale_jitter_lo': 0.5,\n",
            " 'scale_jitter_up': 1.25,\n",
            " 'scoremap_dir': 'test',\n",
            " 'shuffle': True,\n",
            " 'snapshot_prefix': '/content/drive/My '\n",
            "                    'Drive/test-dlc-2023-07-06/dlc-models/iteration-0/testJul6-trainset95shuffle1/train/snapshot',\n",
            " 'stride': 8.0,\n",
            " 'weigh_negatives': False,\n",
            " 'weigh_only_present_joints': False,\n",
            " 'weigh_part_predictions': False,\n",
            " 'weight_decay': 0.0001}\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  warnings.warn('`layer.apply` is deprecated and '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Size is 1\n",
            "Loading ImageNet-pretrained resnet_50\n",
            "Display_iters overwritten as 10\n",
            "Save_iters overwritten as 500\n",
            "Training parameter:\n",
            "{'stride': 8.0, 'weigh_part_predictions': False, 'weigh_negatives': False, 'fg_fraction': 0.25, 'mean_pixel': [123.68, 116.779, 103.939], 'shuffle': True, 'snapshot_prefix': '/content/drive/My Drive/test-dlc-2023-07-06/dlc-models/iteration-0/testJul6-trainset95shuffle1/train/snapshot', 'log_dir': 'log', 'global_scale': 0.8, 'location_refinement': True, 'locref_stdev': 7.2801, 'locref_loss_weight': 0.05, 'locref_huber_loss': True, 'optimizer': 'sgd', 'intermediate_supervision': False, 'intermediate_supervision_layer': 12, 'regularize': False, 'weight_decay': 0.0001, 'crop_pad': 0, 'scoremap_dir': 'test', 'batch_size': 1, 'dataset_type': 'imgaug', 'deterministic': False, 'mirror': False, 'pairwise_huber_loss': False, 'weigh_only_present_joints': False, 'partaffinityfield_predict': False, 'pairwise_predict': False, 'all_joints': [[0], [1], [2], [3]], 'all_joints_names': ['left eye', 'right eye', 'lower head', 'tail'], 'alpha_r': 0.02, 'apply_prob': 0.5, 'contrast': {'clahe': True, 'claheratio': 0.1, 'histeq': True, 'histeqratio': 0.1, 'gamma': False, 'sigmoid': False, 'log': False, 'linear': False}, 'convolution': {'edge': False, 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]}, 'embossratio': 0.1, 'sharpen': False, 'sharpenratio': 0.3}, 'cropratio': 0.4, 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_testJul6/test_dlc95shuffle1.mat', 'decay_steps': 30000, 'display_iters': 1000, 'init_weights': '/usr/local/lib/python3.10/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt', 'lr_init': 0.0005, 'max_input_size': 1500, 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_testJul6/Documentation_data-test_95shuffle1.pickle', 'min_input_size': 64, 'multi_stage': False, 'multi_step': [[0.005, 10000], [0.02, 430000], [0.002, 730000], [0.001, 1030000]], 'net_type': 'resnet_50', 'num_joints': 4, 'pos_dist_thresh': 17, 'project_path': '/content/drive/My Drive/test-dlc-2023-07-06', 'rotation': 25, 'rotratio': 0.4, 'save_iters': 50000, 'scale_jitter_lo': 0.5, 'scale_jitter_up': 1.25, 'covering': True, 'elastic_transform': True, 'motion_blur': True, 'motion_blur_params': {'k': 7, 'angle': (-90, 90)}}\n",
            "Starting training....\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "iteration: 10 loss: 0.2998 lr: 0.005\n",
            "iteration: 20 loss: 0.0401 lr: 0.005\n",
            "iteration: 30 loss: 0.0333 lr: 0.005\n",
            "iteration: 40 loss: 0.0354 lr: 0.005\n",
            "iteration: 50 loss: 0.0251 lr: 0.005\n",
            "iteration: 60 loss: 0.0263 lr: 0.005\n",
            "iteration: 70 loss: 0.0231 lr: 0.005\n",
            "iteration: 80 loss: 0.0365 lr: 0.005\n",
            "iteration: 90 loss: 0.0267 lr: 0.005\n",
            "iteration: 100 loss: 0.0302 lr: 0.005\n",
            "iteration: 110 loss: 0.0264 lr: 0.005\n",
            "iteration: 120 loss: 0.0223 lr: 0.005\n",
            "iteration: 130 loss: 0.0269 lr: 0.005\n",
            "iteration: 140 loss: 0.0228 lr: 0.005\n",
            "iteration: 150 loss: 0.0243 lr: 0.005\n",
            "iteration: 160 loss: 0.0279 lr: 0.005\n",
            "iteration: 170 loss: 0.0186 lr: 0.005\n",
            "iteration: 180 loss: 0.0265 lr: 0.005\n",
            "iteration: 190 loss: 0.0232 lr: 0.005\n",
            "iteration: 200 loss: 0.0203 lr: 0.005\n",
            "iteration: 210 loss: 0.0208 lr: 0.005\n",
            "iteration: 220 loss: 0.0248 lr: 0.005\n",
            "iteration: 230 loss: 0.0204 lr: 0.005\n",
            "iteration: 240 loss: 0.0226 lr: 0.005\n",
            "iteration: 250 loss: 0.0229 lr: 0.005\n",
            "iteration: 260 loss: 0.0269 lr: 0.005\n",
            "iteration: 270 loss: 0.0224 lr: 0.005\n",
            "iteration: 280 loss: 0.0205 lr: 0.005\n",
            "iteration: 290 loss: 0.0289 lr: 0.005\n",
            "iteration: 300 loss: 0.0223 lr: 0.005\n",
            "iteration: 310 loss: 0.0171 lr: 0.005\n",
            "iteration: 320 loss: 0.0189 lr: 0.005\n",
            "iteration: 330 loss: 0.0179 lr: 0.005\n",
            "iteration: 340 loss: 0.0164 lr: 0.005\n",
            "iteration: 350 loss: 0.0196 lr: 0.005\n",
            "iteration: 360 loss: 0.0231 lr: 0.005\n",
            "iteration: 370 loss: 0.0185 lr: 0.005\n",
            "iteration: 380 loss: 0.0161 lr: 0.005\n",
            "iteration: 390 loss: 0.0135 lr: 0.005\n",
            "iteration: 400 loss: 0.0199 lr: 0.005\n",
            "iteration: 410 loss: 0.0196 lr: 0.005\n",
            "iteration: 420 loss: 0.0198 lr: 0.005\n",
            "iteration: 430 loss: 0.0138 lr: 0.005\n",
            "iteration: 440 loss: 0.0177 lr: 0.005\n",
            "iteration: 450 loss: 0.0214 lr: 0.005\n",
            "iteration: 460 loss: 0.0164 lr: 0.005\n",
            "iteration: 470 loss: 0.0167 lr: 0.005\n",
            "iteration: 480 loss: 0.0195 lr: 0.005\n",
            "iteration: 490 loss: 0.0179 lr: 0.005\n",
            "iteration: 500 loss: 0.0161 lr: 0.005\n",
            "iteration: 510 loss: 0.0147 lr: 0.005\n",
            "iteration: 520 loss: 0.0176 lr: 0.005\n",
            "iteration: 530 loss: 0.0205 lr: 0.005\n",
            "iteration: 540 loss: 0.0174 lr: 0.005\n",
            "iteration: 550 loss: 0.0263 lr: 0.005\n",
            "iteration: 560 loss: 0.0169 lr: 0.005\n",
            "iteration: 570 loss: 0.0244 lr: 0.005\n",
            "iteration: 580 loss: 0.0164 lr: 0.005\n",
            "iteration: 590 loss: 0.0186 lr: 0.005\n",
            "iteration: 600 loss: 0.0160 lr: 0.005\n",
            "iteration: 610 loss: 0.0158 lr: 0.005\n",
            "iteration: 620 loss: 0.0139 lr: 0.005\n",
            "iteration: 630 loss: 0.0165 lr: 0.005\n",
            "iteration: 640 loss: 0.0148 lr: 0.005\n",
            "iteration: 650 loss: 0.0150 lr: 0.005\n",
            "iteration: 660 loss: 0.0142 lr: 0.005\n",
            "iteration: 670 loss: 0.0155 lr: 0.005\n",
            "iteration: 680 loss: 0.0151 lr: 0.005\n",
            "iteration: 690 loss: 0.0186 lr: 0.005\n",
            "iteration: 700 loss: 0.0126 lr: 0.005\n",
            "iteration: 710 loss: 0.0178 lr: 0.005\n",
            "iteration: 720 loss: 0.0152 lr: 0.005\n",
            "iteration: 730 loss: 0.0134 lr: 0.005\n",
            "iteration: 740 loss: 0.0137 lr: 0.005\n",
            "iteration: 750 loss: 0.0144 lr: 0.005\n",
            "iteration: 760 loss: 0.0119 lr: 0.005\n",
            "iteration: 770 loss: 0.0193 lr: 0.005\n",
            "iteration: 780 loss: 0.0159 lr: 0.005\n",
            "iteration: 790 loss: 0.0133 lr: 0.005\n",
            "iteration: 800 loss: 0.0177 lr: 0.005\n",
            "iteration: 810 loss: 0.0140 lr: 0.005\n",
            "iteration: 820 loss: 0.0151 lr: 0.005\n",
            "iteration: 830 loss: 0.0152 lr: 0.005\n",
            "iteration: 840 loss: 0.0126 lr: 0.005\n",
            "iteration: 850 loss: 0.0139 lr: 0.005\n",
            "iteration: 860 loss: 0.0116 lr: 0.005\n",
            "iteration: 870 loss: 0.0195 lr: 0.005\n",
            "iteration: 880 loss: 0.0145 lr: 0.005\n",
            "iteration: 890 loss: 0.0154 lr: 0.005\n",
            "iteration: 900 loss: 0.0146 lr: 0.005\n",
            "iteration: 910 loss: 0.0131 lr: 0.005\n",
            "iteration: 920 loss: 0.0130 lr: 0.005\n",
            "iteration: 930 loss: 0.0122 lr: 0.005\n",
            "iteration: 940 loss: 0.0131 lr: 0.005\n",
            "iteration: 950 loss: 0.0134 lr: 0.005\n",
            "iteration: 960 loss: 0.0119 lr: 0.005\n",
            "iteration: 970 loss: 0.0162 lr: 0.005\n",
            "iteration: 980 loss: 0.0189 lr: 0.005\n",
            "iteration: 990 loss: 0.0119 lr: 0.005\n",
            "iteration: 1000 loss: 0.0177 lr: 0.005\n",
            "iteration: 1010 loss: 0.0117 lr: 0.005\n",
            "iteration: 1020 loss: 0.0168 lr: 0.005\n",
            "iteration: 1030 loss: 0.0162 lr: 0.005\n",
            "iteration: 1040 loss: 0.0152 lr: 0.005\n",
            "iteration: 1050 loss: 0.0109 lr: 0.005\n",
            "iteration: 1060 loss: 0.0140 lr: 0.005\n",
            "iteration: 1070 loss: 0.0118 lr: 0.005\n",
            "iteration: 1080 loss: 0.0125 lr: 0.005\n",
            "iteration: 1090 loss: 0.0100 lr: 0.005\n",
            "iteration: 1100 loss: 0.0148 lr: 0.005\n",
            "iteration: 1110 loss: 0.0093 lr: 0.005\n",
            "iteration: 1120 loss: 0.0116 lr: 0.005\n",
            "iteration: 1130 loss: 0.0186 lr: 0.005\n",
            "iteration: 1140 loss: 0.0117 lr: 0.005\n",
            "iteration: 1150 loss: 0.0164 lr: 0.005\n",
            "iteration: 1160 loss: 0.0116 lr: 0.005\n",
            "iteration: 1170 loss: 0.0122 lr: 0.005\n",
            "iteration: 1180 loss: 0.0124 lr: 0.005\n",
            "iteration: 1190 loss: 0.0118 lr: 0.005\n",
            "iteration: 1200 loss: 0.0117 lr: 0.005\n",
            "iteration: 1210 loss: 0.0107 lr: 0.005\n",
            "iteration: 1220 loss: 0.0141 lr: 0.005\n",
            "iteration: 1230 loss: 0.0100 lr: 0.005\n",
            "iteration: 1240 loss: 0.0076 lr: 0.005\n",
            "iteration: 1250 loss: 0.0118 lr: 0.005\n",
            "iteration: 1260 loss: 0.0112 lr: 0.005\n",
            "iteration: 1270 loss: 0.0096 lr: 0.005\n",
            "iteration: 1280 loss: 0.0123 lr: 0.005\n",
            "iteration: 1290 loss: 0.0130 lr: 0.005\n",
            "iteration: 1300 loss: 0.0113 lr: 0.005\n",
            "iteration: 1310 loss: 0.0124 lr: 0.005\n",
            "iteration: 1320 loss: 0.0133 lr: 0.005\n",
            "iteration: 1330 loss: 0.0134 lr: 0.005\n",
            "iteration: 1340 loss: 0.0109 lr: 0.005\n",
            "iteration: 1350 loss: 0.0123 lr: 0.005\n",
            "iteration: 1360 loss: 0.0112 lr: 0.005\n",
            "iteration: 1370 loss: 0.0113 lr: 0.005\n",
            "iteration: 1380 loss: 0.0087 lr: 0.005\n",
            "iteration: 1390 loss: 0.0123 lr: 0.005\n",
            "iteration: 1400 loss: 0.0156 lr: 0.005\n",
            "iteration: 1410 loss: 0.0121 lr: 0.005\n",
            "iteration: 1420 loss: 0.0101 lr: 0.005\n",
            "iteration: 1430 loss: 0.0097 lr: 0.005\n",
            "iteration: 1440 loss: 0.0105 lr: 0.005\n",
            "iteration: 1450 loss: 0.0116 lr: 0.005\n",
            "iteration: 1460 loss: 0.0082 lr: 0.005\n",
            "iteration: 1470 loss: 0.0140 lr: 0.005\n",
            "iteration: 1480 loss: 0.0086 lr: 0.005\n",
            "iteration: 1490 loss: 0.0098 lr: 0.005\n",
            "iteration: 1500 loss: 0.0128 lr: 0.005\n",
            "iteration: 1510 loss: 0.0087 lr: 0.005\n",
            "iteration: 1520 loss: 0.0101 lr: 0.005\n",
            "iteration: 1530 loss: 0.0109 lr: 0.005\n",
            "iteration: 1540 loss: 0.0127 lr: 0.005\n",
            "iteration: 1550 loss: 0.0092 lr: 0.005\n",
            "iteration: 1560 loss: 0.0094 lr: 0.005\n",
            "iteration: 1570 loss: 0.0109 lr: 0.005\n",
            "iteration: 1580 loss: 0.0121 lr: 0.005\n",
            "iteration: 1590 loss: 0.0118 lr: 0.005\n",
            "iteration: 1600 loss: 0.0082 lr: 0.005\n",
            "iteration: 1610 loss: 0.0079 lr: 0.005\n",
            "iteration: 1620 loss: 0.0111 lr: 0.005\n",
            "iteration: 1630 loss: 0.0078 lr: 0.005\n",
            "iteration: 1640 loss: 0.0090 lr: 0.005\n",
            "iteration: 1650 loss: 0.0104 lr: 0.005\n",
            "iteration: 1660 loss: 0.0081 lr: 0.005\n",
            "iteration: 1670 loss: 0.0097 lr: 0.005\n",
            "iteration: 1680 loss: 0.0105 lr: 0.005\n",
            "iteration: 1690 loss: 0.0109 lr: 0.005\n",
            "iteration: 1700 loss: 0.0115 lr: 0.005\n",
            "iteration: 1710 loss: 0.0086 lr: 0.005\n",
            "iteration: 1720 loss: 0.0087 lr: 0.005\n",
            "iteration: 1730 loss: 0.0091 lr: 0.005\n",
            "iteration: 1740 loss: 0.0082 lr: 0.005\n",
            "iteration: 1750 loss: 0.0091 lr: 0.005\n",
            "iteration: 1760 loss: 0.0104 lr: 0.005\n",
            "iteration: 1770 loss: 0.0085 lr: 0.005\n",
            "iteration: 1780 loss: 0.0127 lr: 0.005\n",
            "iteration: 1790 loss: 0.0098 lr: 0.005\n",
            "iteration: 1800 loss: 0.0077 lr: 0.005\n",
            "iteration: 1810 loss: 0.0120 lr: 0.005\n",
            "iteration: 1820 loss: 0.0100 lr: 0.005\n",
            "iteration: 1830 loss: 0.0099 lr: 0.005\n",
            "iteration: 1840 loss: 0.0112 lr: 0.005\n",
            "iteration: 1850 loss: 0.0100 lr: 0.005\n",
            "iteration: 1860 loss: 0.0094 lr: 0.005\n",
            "iteration: 1870 loss: 0.0078 lr: 0.005\n",
            "iteration: 1880 loss: 0.0086 lr: 0.005\n",
            "iteration: 1890 loss: 0.0109 lr: 0.005\n",
            "iteration: 1900 loss: 0.0086 lr: 0.005\n",
            "iteration: 1910 loss: 0.0075 lr: 0.005\n",
            "iteration: 1920 loss: 0.0088 lr: 0.005\n",
            "iteration: 1930 loss: 0.0117 lr: 0.005\n",
            "iteration: 1940 loss: 0.0141 lr: 0.005\n",
            "iteration: 1950 loss: 0.0100 lr: 0.005\n",
            "iteration: 1960 loss: 0.0091 lr: 0.005\n",
            "iteration: 1970 loss: 0.0091 lr: 0.005\n",
            "iteration: 1980 loss: 0.0108 lr: 0.005\n",
            "iteration: 1990 loss: 0.0125 lr: 0.005\n",
            "iteration: 2000 loss: 0.0076 lr: 0.005\n",
            "iteration: 2010 loss: 0.0086 lr: 0.005\n",
            "iteration: 2020 loss: 0.0081 lr: 0.005\n",
            "iteration: 2030 loss: 0.0086 lr: 0.005\n",
            "iteration: 2040 loss: 0.0087 lr: 0.005\n",
            "iteration: 2050 loss: 0.0069 lr: 0.005\n",
            "iteration: 2060 loss: 0.0086 lr: 0.005\n",
            "iteration: 2070 loss: 0.0088 lr: 0.005\n",
            "iteration: 2080 loss: 0.0079 lr: 0.005\n",
            "iteration: 2090 loss: 0.0074 lr: 0.005\n",
            "iteration: 2100 loss: 0.0086 lr: 0.005\n",
            "iteration: 2110 loss: 0.0100 lr: 0.005\n",
            "iteration: 2120 loss: 0.0096 lr: 0.005\n",
            "iteration: 2130 loss: 0.0087 lr: 0.005\n",
            "iteration: 2140 loss: 0.0097 lr: 0.005\n",
            "iteration: 2150 loss: 0.0093 lr: 0.005\n",
            "iteration: 2160 loss: 0.0090 lr: 0.005\n",
            "iteration: 2170 loss: 0.0076 lr: 0.005\n",
            "iteration: 2180 loss: 0.0108 lr: 0.005\n",
            "iteration: 2190 loss: 0.0095 lr: 0.005\n",
            "iteration: 2200 loss: 0.0080 lr: 0.005\n",
            "iteration: 2210 loss: 0.0084 lr: 0.005\n",
            "iteration: 2220 loss: 0.0085 lr: 0.005\n",
            "iteration: 2230 loss: 0.0083 lr: 0.005\n",
            "iteration: 2240 loss: 0.0077 lr: 0.005\n",
            "iteration: 2250 loss: 0.0069 lr: 0.005\n",
            "iteration: 2260 loss: 0.0066 lr: 0.005\n",
            "iteration: 2270 loss: 0.0091 lr: 0.005\n",
            "iteration: 2280 loss: 0.0086 lr: 0.005\n",
            "iteration: 2290 loss: 0.0074 lr: 0.005\n",
            "iteration: 2300 loss: 0.0088 lr: 0.005\n",
            "iteration: 2310 loss: 0.0104 lr: 0.005\n",
            "iteration: 2320 loss: 0.0086 lr: 0.005\n",
            "iteration: 2330 loss: 0.0083 lr: 0.005\n",
            "iteration: 2340 loss: 0.0085 lr: 0.005\n",
            "iteration: 2350 loss: 0.0090 lr: 0.005\n",
            "iteration: 2360 loss: 0.0068 lr: 0.005\n",
            "iteration: 2370 loss: 0.0094 lr: 0.005\n",
            "iteration: 2380 loss: 0.0070 lr: 0.005\n",
            "iteration: 2390 loss: 0.0078 lr: 0.005\n",
            "iteration: 2400 loss: 0.0075 lr: 0.005\n",
            "iteration: 2410 loss: 0.0082 lr: 0.005\n",
            "iteration: 2420 loss: 0.0095 lr: 0.005\n",
            "iteration: 2430 loss: 0.0094 lr: 0.005\n",
            "iteration: 2440 loss: 0.0094 lr: 0.005\n",
            "iteration: 2450 loss: 0.0111 lr: 0.005\n",
            "iteration: 2460 loss: 0.0071 lr: 0.005\n",
            "iteration: 2470 loss: 0.0072 lr: 0.005\n",
            "iteration: 2480 loss: 0.0089 lr: 0.005\n",
            "iteration: 2490 loss: 0.0095 lr: 0.005\n",
            "iteration: 2500 loss: 0.0089 lr: 0.005\n",
            "iteration: 2510 loss: 0.0068 lr: 0.005\n",
            "iteration: 2520 loss: 0.0084 lr: 0.005\n",
            "iteration: 2530 loss: 0.0078 lr: 0.005\n",
            "iteration: 2540 loss: 0.0090 lr: 0.005\n",
            "iteration: 2550 loss: 0.0098 lr: 0.005\n",
            "iteration: 2560 loss: 0.0095 lr: 0.005\n",
            "iteration: 2570 loss: 0.0078 lr: 0.005\n",
            "iteration: 2580 loss: 0.0079 lr: 0.005\n",
            "iteration: 2590 loss: 0.0077 lr: 0.005\n",
            "iteration: 2600 loss: 0.0051 lr: 0.005\n",
            "iteration: 2610 loss: 0.0064 lr: 0.005\n",
            "iteration: 2620 loss: 0.0078 lr: 0.005\n",
            "iteration: 2630 loss: 0.0076 lr: 0.005\n",
            "iteration: 2640 loss: 0.0083 lr: 0.005\n",
            "iteration: 2650 loss: 0.0070 lr: 0.005\n",
            "iteration: 2660 loss: 0.0066 lr: 0.005\n",
            "iteration: 2670 loss: 0.0066 lr: 0.005\n",
            "iteration: 2680 loss: 0.0105 lr: 0.005\n",
            "iteration: 2690 loss: 0.0096 lr: 0.005\n",
            "iteration: 2700 loss: 0.0064 lr: 0.005\n",
            "iteration: 2710 loss: 0.0077 lr: 0.005\n",
            "iteration: 2720 loss: 0.0063 lr: 0.005\n",
            "iteration: 2730 loss: 0.0081 lr: 0.005\n",
            "iteration: 2740 loss: 0.0073 lr: 0.005\n",
            "iteration: 2750 loss: 0.0081 lr: 0.005\n",
            "iteration: 2760 loss: 0.0087 lr: 0.005\n",
            "iteration: 2770 loss: 0.0060 lr: 0.005\n",
            "iteration: 2780 loss: 0.0063 lr: 0.005\n",
            "iteration: 2790 loss: 0.0072 lr: 0.005\n",
            "iteration: 2800 loss: 0.0072 lr: 0.005\n",
            "iteration: 2810 loss: 0.0064 lr: 0.005\n",
            "iteration: 2820 loss: 0.0071 lr: 0.005\n",
            "iteration: 2830 loss: 0.0061 lr: 0.005\n",
            "iteration: 2840 loss: 0.0097 lr: 0.005\n",
            "iteration: 2850 loss: 0.0074 lr: 0.005\n",
            "iteration: 2860 loss: 0.0054 lr: 0.005\n",
            "iteration: 2870 loss: 0.0062 lr: 0.005\n",
            "iteration: 2880 loss: 0.0073 lr: 0.005\n",
            "iteration: 2890 loss: 0.0087 lr: 0.005\n",
            "iteration: 2900 loss: 0.0099 lr: 0.005\n",
            "iteration: 2910 loss: 0.0087 lr: 0.005\n",
            "iteration: 2920 loss: 0.0071 lr: 0.005\n",
            "iteration: 2930 loss: 0.0085 lr: 0.005\n",
            "iteration: 2940 loss: 0.0072 lr: 0.005\n",
            "iteration: 2950 loss: 0.0070 lr: 0.005\n",
            "iteration: 2960 loss: 0.0095 lr: 0.005\n",
            "iteration: 2970 loss: 0.0083 lr: 0.005\n",
            "iteration: 2980 loss: 0.0081 lr: 0.005\n",
            "iteration: 2990 loss: 0.0077 lr: 0.005\n",
            "iteration: 3000 loss: 0.0072 lr: 0.005\n",
            "iteration: 3010 loss: 0.0079 lr: 0.005\n",
            "iteration: 3020 loss: 0.0090 lr: 0.005\n",
            "iteration: 3030 loss: 0.0061 lr: 0.005\n",
            "iteration: 3040 loss: 0.0071 lr: 0.005\n",
            "iteration: 3050 loss: 0.0060 lr: 0.005\n",
            "iteration: 3060 loss: 0.0092 lr: 0.005\n",
            "iteration: 3070 loss: 0.0069 lr: 0.005\n",
            "iteration: 3080 loss: 0.0071 lr: 0.005\n",
            "iteration: 3090 loss: 0.0067 lr: 0.005\n",
            "iteration: 3100 loss: 0.0066 lr: 0.005\n",
            "iteration: 3110 loss: 0.0064 lr: 0.005\n",
            "iteration: 3120 loss: 0.0087 lr: 0.005\n",
            "iteration: 3130 loss: 0.0066 lr: 0.005\n",
            "iteration: 3140 loss: 0.0109 lr: 0.005\n",
            "iteration: 3150 loss: 0.0060 lr: 0.005\n",
            "iteration: 3160 loss: 0.0069 lr: 0.005\n",
            "iteration: 3170 loss: 0.0064 lr: 0.005\n",
            "iteration: 3180 loss: 0.0071 lr: 0.005\n",
            "iteration: 3190 loss: 0.0062 lr: 0.005\n",
            "iteration: 3200 loss: 0.0115 lr: 0.005\n",
            "iteration: 3210 loss: 0.0060 lr: 0.005\n",
            "iteration: 3220 loss: 0.0049 lr: 0.005\n",
            "iteration: 3230 loss: 0.0085 lr: 0.005\n",
            "iteration: 3240 loss: 0.0110 lr: 0.005\n",
            "iteration: 3250 loss: 0.0066 lr: 0.005\n",
            "iteration: 3260 loss: 0.0075 lr: 0.005\n",
            "iteration: 3270 loss: 0.0079 lr: 0.005\n",
            "iteration: 3280 loss: 0.0066 lr: 0.005\n",
            "iteration: 3290 loss: 0.0070 lr: 0.005\n",
            "iteration: 3300 loss: 0.0090 lr: 0.005\n",
            "iteration: 3310 loss: 0.0069 lr: 0.005\n",
            "iteration: 3320 loss: 0.0060 lr: 0.005\n",
            "iteration: 3330 loss: 0.0052 lr: 0.005\n",
            "iteration: 3340 loss: 0.0070 lr: 0.005\n",
            "iteration: 3350 loss: 0.0060 lr: 0.005\n",
            "iteration: 3360 loss: 0.0059 lr: 0.005\n",
            "iteration: 3370 loss: 0.0062 lr: 0.005\n",
            "iteration: 3380 loss: 0.0048 lr: 0.005\n",
            "iteration: 3390 loss: 0.0051 lr: 0.005\n",
            "iteration: 3400 loss: 0.0064 lr: 0.005\n",
            "iteration: 3410 loss: 0.0082 lr: 0.005\n",
            "iteration: 3420 loss: 0.0080 lr: 0.005\n",
            "iteration: 3430 loss: 0.0062 lr: 0.005\n",
            "iteration: 3440 loss: 0.0069 lr: 0.005\n",
            "iteration: 3450 loss: 0.0051 lr: 0.005\n",
            "iteration: 3460 loss: 0.0055 lr: 0.005\n",
            "iteration: 3470 loss: 0.0050 lr: 0.005\n",
            "iteration: 3480 loss: 0.0055 lr: 0.005\n",
            "iteration: 3490 loss: 0.0062 lr: 0.005\n",
            "iteration: 3500 loss: 0.0057 lr: 0.005\n",
            "iteration: 3510 loss: 0.0059 lr: 0.005\n",
            "iteration: 3520 loss: 0.0071 lr: 0.005\n",
            "iteration: 3530 loss: 0.0094 lr: 0.005\n",
            "iteration: 3540 loss: 0.0061 lr: 0.005\n",
            "iteration: 3550 loss: 0.0054 lr: 0.005\n",
            "iteration: 3560 loss: 0.0039 lr: 0.005\n",
            "iteration: 3570 loss: 0.0051 lr: 0.005\n",
            "iteration: 3580 loss: 0.0064 lr: 0.005\n",
            "iteration: 3590 loss: 0.0086 lr: 0.005\n",
            "iteration: 3600 loss: 0.0072 lr: 0.005\n",
            "iteration: 3610 loss: 0.0082 lr: 0.005\n",
            "iteration: 3620 loss: 0.0089 lr: 0.005\n",
            "iteration: 3630 loss: 0.0070 lr: 0.005\n",
            "iteration: 3640 loss: 0.0053 lr: 0.005\n",
            "iteration: 3650 loss: 0.0074 lr: 0.005\n",
            "iteration: 3660 loss: 0.0077 lr: 0.005\n",
            "iteration: 3670 loss: 0.0079 lr: 0.005\n",
            "iteration: 3680 loss: 0.0077 lr: 0.005\n",
            "iteration: 3690 loss: 0.0106 lr: 0.005\n",
            "iteration: 3700 loss: 0.0070 lr: 0.005\n",
            "iteration: 3710 loss: 0.0049 lr: 0.005\n",
            "iteration: 3720 loss: 0.0072 lr: 0.005\n",
            "iteration: 3730 loss: 0.0067 lr: 0.005\n",
            "iteration: 3740 loss: 0.0057 lr: 0.005\n",
            "iteration: 3750 loss: 0.0056 lr: 0.005\n",
            "iteration: 3760 loss: 0.0074 lr: 0.005\n",
            "iteration: 3770 loss: 0.0097 lr: 0.005\n",
            "iteration: 3780 loss: 0.0068 lr: 0.005\n",
            "iteration: 3790 loss: 0.0056 lr: 0.005\n",
            "iteration: 3800 loss: 0.0080 lr: 0.005\n",
            "iteration: 3810 loss: 0.0081 lr: 0.005\n",
            "iteration: 3820 loss: 0.0056 lr: 0.005\n",
            "iteration: 3830 loss: 0.0062 lr: 0.005\n",
            "iteration: 3840 loss: 0.0079 lr: 0.005\n",
            "iteration: 3850 loss: 0.0075 lr: 0.005\n",
            "iteration: 3860 loss: 0.0064 lr: 0.005\n",
            "iteration: 3870 loss: 0.0064 lr: 0.005\n",
            "iteration: 3880 loss: 0.0080 lr: 0.005\n",
            "iteration: 3890 loss: 0.0105 lr: 0.005\n",
            "iteration: 3900 loss: 0.0049 lr: 0.005\n",
            "iteration: 3910 loss: 0.0079 lr: 0.005\n",
            "iteration: 3920 loss: 0.0072 lr: 0.005\n",
            "iteration: 3930 loss: 0.0052 lr: 0.005\n",
            "iteration: 3940 loss: 0.0057 lr: 0.005\n",
            "iteration: 3950 loss: 0.0067 lr: 0.005\n",
            "iteration: 3960 loss: 0.0073 lr: 0.005\n",
            "iteration: 3970 loss: 0.0069 lr: 0.005\n",
            "iteration: 3980 loss: 0.0060 lr: 0.005\n",
            "iteration: 3990 loss: 0.0057 lr: 0.005\n",
            "iteration: 4000 loss: 0.0078 lr: 0.005\n",
            "iteration: 4010 loss: 0.0063 lr: 0.005\n",
            "iteration: 4020 loss: 0.0054 lr: 0.005\n",
            "iteration: 4030 loss: 0.0085 lr: 0.005\n",
            "iteration: 4040 loss: 0.0058 lr: 0.005\n",
            "iteration: 4050 loss: 0.0074 lr: 0.005\n",
            "iteration: 4060 loss: 0.0055 lr: 0.005\n",
            "iteration: 4070 loss: 0.0045 lr: 0.005\n",
            "iteration: 4080 loss: 0.0049 lr: 0.005\n",
            "iteration: 4090 loss: 0.0059 lr: 0.005\n",
            "iteration: 4100 loss: 0.0073 lr: 0.005\n",
            "iteration: 4110 loss: 0.0084 lr: 0.005\n",
            "iteration: 4120 loss: 0.0061 lr: 0.005\n",
            "iteration: 4130 loss: 0.0059 lr: 0.005\n",
            "iteration: 4140 loss: 0.0058 lr: 0.005\n",
            "iteration: 4150 loss: 0.0053 lr: 0.005\n",
            "iteration: 4160 loss: 0.0049 lr: 0.005\n",
            "iteration: 4170 loss: 0.0066 lr: 0.005\n",
            "iteration: 4180 loss: 0.0072 lr: 0.005\n",
            "iteration: 4190 loss: 0.0051 lr: 0.005\n",
            "iteration: 4200 loss: 0.0049 lr: 0.005\n",
            "iteration: 4210 loss: 0.0071 lr: 0.005\n",
            "iteration: 4220 loss: 0.0063 lr: 0.005\n",
            "iteration: 4230 loss: 0.0056 lr: 0.005\n",
            "iteration: 4240 loss: 0.0055 lr: 0.005\n",
            "iteration: 4250 loss: 0.0071 lr: 0.005\n",
            "iteration: 4260 loss: 0.0082 lr: 0.005\n",
            "iteration: 4270 loss: 0.0060 lr: 0.005\n",
            "iteration: 4280 loss: 0.0060 lr: 0.005\n",
            "iteration: 4290 loss: 0.0056 lr: 0.005\n",
            "iteration: 4300 loss: 0.0085 lr: 0.005\n",
            "iteration: 4310 loss: 0.0055 lr: 0.005\n",
            "iteration: 4320 loss: 0.0063 lr: 0.005\n",
            "iteration: 4330 loss: 0.0061 lr: 0.005\n",
            "iteration: 4340 loss: 0.0066 lr: 0.005\n",
            "iteration: 4350 loss: 0.0053 lr: 0.005\n",
            "iteration: 4360 loss: 0.0047 lr: 0.005\n",
            "iteration: 4370 loss: 0.0044 lr: 0.005\n",
            "iteration: 4380 loss: 0.0088 lr: 0.005\n",
            "iteration: 4390 loss: 0.0069 lr: 0.005\n",
            "iteration: 4400 loss: 0.0070 lr: 0.005\n",
            "iteration: 4410 loss: 0.0070 lr: 0.005\n",
            "iteration: 4420 loss: 0.0058 lr: 0.005\n",
            "iteration: 4430 loss: 0.0063 lr: 0.005\n",
            "iteration: 4440 loss: 0.0057 lr: 0.005\n",
            "iteration: 4450 loss: 0.0064 lr: 0.005\n",
            "iteration: 4460 loss: 0.0062 lr: 0.005\n",
            "iteration: 4470 loss: 0.0052 lr: 0.005\n",
            "iteration: 4480 loss: 0.0048 lr: 0.005\n",
            "iteration: 4490 loss: 0.0045 lr: 0.005\n",
            "iteration: 4500 loss: 0.0067 lr: 0.005\n",
            "iteration: 4510 loss: 0.0069 lr: 0.005\n",
            "iteration: 4520 loss: 0.0051 lr: 0.005\n",
            "iteration: 4530 loss: 0.0046 lr: 0.005\n",
            "iteration: 4540 loss: 0.0057 lr: 0.005\n",
            "iteration: 4550 loss: 0.0048 lr: 0.005\n",
            "iteration: 4560 loss: 0.0055 lr: 0.005\n",
            "iteration: 4570 loss: 0.0048 lr: 0.005\n",
            "iteration: 4580 loss: 0.0050 lr: 0.005\n",
            "iteration: 4590 loss: 0.0053 lr: 0.005\n",
            "iteration: 4600 loss: 0.0064 lr: 0.005\n",
            "iteration: 4610 loss: 0.0052 lr: 0.005\n",
            "iteration: 4620 loss: 0.0050 lr: 0.005\n",
            "iteration: 4630 loss: 0.0053 lr: 0.005\n",
            "iteration: 4640 loss: 0.0053 lr: 0.005\n",
            "iteration: 4650 loss: 0.0081 lr: 0.005\n",
            "iteration: 4660 loss: 0.0065 lr: 0.005\n",
            "iteration: 4670 loss: 0.0066 lr: 0.005\n",
            "iteration: 4680 loss: 0.0071 lr: 0.005\n",
            "iteration: 4690 loss: 0.0058 lr: 0.005\n",
            "iteration: 4700 loss: 0.0060 lr: 0.005\n",
            "iteration: 4710 loss: 0.0043 lr: 0.005\n",
            "iteration: 4720 loss: 0.0053 lr: 0.005\n",
            "iteration: 4730 loss: 0.0060 lr: 0.005\n",
            "iteration: 4740 loss: 0.0045 lr: 0.005\n",
            "iteration: 4750 loss: 0.0054 lr: 0.005\n",
            "iteration: 4760 loss: 0.0059 lr: 0.005\n",
            "iteration: 4770 loss: 0.0066 lr: 0.005\n",
            "iteration: 4780 loss: 0.0101 lr: 0.005\n",
            "iteration: 4790 loss: 0.0050 lr: 0.005\n",
            "iteration: 4800 loss: 0.0057 lr: 0.005\n",
            "iteration: 4810 loss: 0.0054 lr: 0.005\n",
            "iteration: 4820 loss: 0.0077 lr: 0.005\n",
            "iteration: 4830 loss: 0.0053 lr: 0.005\n",
            "iteration: 4840 loss: 0.0056 lr: 0.005\n",
            "iteration: 4850 loss: 0.0057 lr: 0.005\n",
            "iteration: 4860 loss: 0.0062 lr: 0.005\n",
            "iteration: 4870 loss: 0.0055 lr: 0.005\n",
            "iteration: 4880 loss: 0.0068 lr: 0.005\n",
            "iteration: 4890 loss: 0.0063 lr: 0.005\n",
            "iteration: 4900 loss: 0.0052 lr: 0.005\n",
            "iteration: 4910 loss: 0.0054 lr: 0.005\n",
            "iteration: 4920 loss: 0.0049 lr: 0.005\n",
            "iteration: 4930 loss: 0.0033 lr: 0.005\n",
            "iteration: 4940 loss: 0.0050 lr: 0.005\n",
            "iteration: 4950 loss: 0.0044 lr: 0.005\n",
            "iteration: 4960 loss: 0.0063 lr: 0.005\n",
            "iteration: 4970 loss: 0.0063 lr: 0.005\n",
            "iteration: 4980 loss: 0.0059 lr: 0.005\n",
            "iteration: 4990 loss: 0.0056 lr: 0.005\n",
            "iteration: 5000 loss: 0.0056 lr: 0.005\n",
            "iteration: 5010 loss: 0.0041 lr: 0.005\n",
            "iteration: 5020 loss: 0.0060 lr: 0.005\n",
            "iteration: 5030 loss: 0.0071 lr: 0.005\n",
            "iteration: 5040 loss: 0.0062 lr: 0.005\n",
            "iteration: 5050 loss: 0.0054 lr: 0.005\n",
            "iteration: 5060 loss: 0.0055 lr: 0.005\n",
            "iteration: 5070 loss: 0.0071 lr: 0.005\n",
            "iteration: 5080 loss: 0.0068 lr: 0.005\n",
            "iteration: 5090 loss: 0.0059 lr: 0.005\n",
            "iteration: 5100 loss: 0.0056 lr: 0.005\n",
            "iteration: 5110 loss: 0.0037 lr: 0.005\n",
            "iteration: 5120 loss: 0.0064 lr: 0.005\n",
            "iteration: 5130 loss: 0.0077 lr: 0.005\n",
            "iteration: 5140 loss: 0.0051 lr: 0.005\n",
            "iteration: 5150 loss: 0.0059 lr: 0.005\n",
            "iteration: 5160 loss: 0.0059 lr: 0.005\n",
            "iteration: 5170 loss: 0.0059 lr: 0.005\n",
            "iteration: 5180 loss: 0.0071 lr: 0.005\n",
            "iteration: 5190 loss: 0.0058 lr: 0.005\n",
            "iteration: 5200 loss: 0.0049 lr: 0.005\n",
            "iteration: 5210 loss: 0.0081 lr: 0.005\n",
            "iteration: 5220 loss: 0.0045 lr: 0.005\n",
            "iteration: 5230 loss: 0.0049 lr: 0.005\n",
            "iteration: 5240 loss: 0.0059 lr: 0.005\n",
            "iteration: 5250 loss: 0.0054 lr: 0.005\n",
            "iteration: 5260 loss: 0.0067 lr: 0.005\n",
            "iteration: 5270 loss: 0.0048 lr: 0.005\n",
            "iteration: 5280 loss: 0.0049 lr: 0.005\n",
            "iteration: 5290 loss: 0.0051 lr: 0.005\n",
            "iteration: 5300 loss: 0.0044 lr: 0.005\n",
            "iteration: 5310 loss: 0.0041 lr: 0.005\n",
            "iteration: 5320 loss: 0.0057 lr: 0.005\n",
            "iteration: 5330 loss: 0.0043 lr: 0.005\n",
            "iteration: 5340 loss: 0.0042 lr: 0.005\n",
            "iteration: 5350 loss: 0.0038 lr: 0.005\n",
            "iteration: 5360 loss: 0.0047 lr: 0.005\n",
            "iteration: 5370 loss: 0.0054 lr: 0.005\n",
            "iteration: 5380 loss: 0.0056 lr: 0.005\n",
            "iteration: 5390 loss: 0.0050 lr: 0.005\n",
            "iteration: 5400 loss: 0.0057 lr: 0.005\n",
            "iteration: 5410 loss: 0.0044 lr: 0.005\n",
            "iteration: 5420 loss: 0.0058 lr: 0.005\n",
            "iteration: 5430 loss: 0.0054 lr: 0.005\n",
            "iteration: 5440 loss: 0.0056 lr: 0.005\n",
            "iteration: 5450 loss: 0.0060 lr: 0.005\n",
            "iteration: 5460 loss: 0.0043 lr: 0.005\n",
            "iteration: 5470 loss: 0.0088 lr: 0.005\n",
            "iteration: 5480 loss: 0.0059 lr: 0.005\n",
            "iteration: 5490 loss: 0.0043 lr: 0.005\n",
            "iteration: 5500 loss: 0.0048 lr: 0.005\n",
            "iteration: 5510 loss: 0.0047 lr: 0.005\n",
            "iteration: 5520 loss: 0.0048 lr: 0.005\n",
            "iteration: 5530 loss: 0.0038 lr: 0.005\n",
            "iteration: 5540 loss: 0.0070 lr: 0.005\n",
            "iteration: 5550 loss: 0.0068 lr: 0.005\n",
            "iteration: 5560 loss: 0.0063 lr: 0.005\n",
            "iteration: 5570 loss: 0.0050 lr: 0.005\n",
            "iteration: 5580 loss: 0.0069 lr: 0.005\n",
            "iteration: 5590 loss: 0.0053 lr: 0.005\n",
            "iteration: 5600 loss: 0.0060 lr: 0.005\n",
            "iteration: 5610 loss: 0.0045 lr: 0.005\n",
            "iteration: 5620 loss: 0.0046 lr: 0.005\n",
            "iteration: 5630 loss: 0.0048 lr: 0.005\n",
            "iteration: 5640 loss: 0.0048 lr: 0.005\n",
            "iteration: 5650 loss: 0.0050 lr: 0.005\n",
            "iteration: 5660 loss: 0.0079 lr: 0.005\n",
            "iteration: 5670 loss: 0.0064 lr: 0.005\n",
            "iteration: 5680 loss: 0.0057 lr: 0.005\n",
            "iteration: 5690 loss: 0.0046 lr: 0.005\n",
            "iteration: 5700 loss: 0.0037 lr: 0.005\n",
            "iteration: 5710 loss: 0.0051 lr: 0.005\n",
            "iteration: 5720 loss: 0.0060 lr: 0.005\n",
            "iteration: 5730 loss: 0.0052 lr: 0.005\n",
            "iteration: 5740 loss: 0.0040 lr: 0.005\n",
            "iteration: 5750 loss: 0.0075 lr: 0.005\n",
            "iteration: 5760 loss: 0.0036 lr: 0.005\n",
            "iteration: 5770 loss: 0.0043 lr: 0.005\n",
            "iteration: 5780 loss: 0.0065 lr: 0.005\n",
            "iteration: 5790 loss: 0.0077 lr: 0.005\n",
            "iteration: 5800 loss: 0.0046 lr: 0.005\n",
            "iteration: 5810 loss: 0.0064 lr: 0.005\n",
            "iteration: 5820 loss: 0.0056 lr: 0.005\n",
            "iteration: 5830 loss: 0.0061 lr: 0.005\n",
            "iteration: 5840 loss: 0.0039 lr: 0.005\n",
            "iteration: 5850 loss: 0.0057 lr: 0.005\n",
            "iteration: 5860 loss: 0.0056 lr: 0.005\n",
            "iteration: 5870 loss: 0.0049 lr: 0.005\n",
            "iteration: 5880 loss: 0.0053 lr: 0.005\n",
            "iteration: 5890 loss: 0.0078 lr: 0.005\n",
            "iteration: 5900 loss: 0.0061 lr: 0.005\n",
            "iteration: 5910 loss: 0.0066 lr: 0.005\n",
            "iteration: 5920 loss: 0.0048 lr: 0.005\n",
            "iteration: 5930 loss: 0.0049 lr: 0.005\n",
            "iteration: 5940 loss: 0.0037 lr: 0.005\n",
            "iteration: 5950 loss: 0.0060 lr: 0.005\n",
            "iteration: 5960 loss: 0.0047 lr: 0.005\n",
            "iteration: 5970 loss: 0.0060 lr: 0.005\n",
            "iteration: 5980 loss: 0.0044 lr: 0.005\n",
            "iteration: 5990 loss: 0.0052 lr: 0.005\n",
            "iteration: 6000 loss: 0.0049 lr: 0.005\n",
            "iteration: 6010 loss: 0.0050 lr: 0.005\n",
            "iteration: 6020 loss: 0.0057 lr: 0.005\n",
            "iteration: 6030 loss: 0.0037 lr: 0.005\n",
            "iteration: 6040 loss: 0.0053 lr: 0.005\n",
            "iteration: 6050 loss: 0.0047 lr: 0.005\n",
            "iteration: 6060 loss: 0.0047 lr: 0.005\n",
            "iteration: 6070 loss: 0.0053 lr: 0.005\n",
            "iteration: 6080 loss: 0.0043 lr: 0.005\n",
            "iteration: 6090 loss: 0.0040 lr: 0.005\n",
            "iteration: 6100 loss: 0.0043 lr: 0.005\n",
            "iteration: 6110 loss: 0.0049 lr: 0.005\n",
            "iteration: 6120 loss: 0.0071 lr: 0.005\n",
            "iteration: 6130 loss: 0.0089 lr: 0.005\n",
            "iteration: 6140 loss: 0.0059 lr: 0.005\n",
            "iteration: 6150 loss: 0.0044 lr: 0.005\n",
            "iteration: 6160 loss: 0.0043 lr: 0.005\n",
            "iteration: 6170 loss: 0.0076 lr: 0.005\n",
            "iteration: 6180 loss: 0.0059 lr: 0.005\n",
            "iteration: 6190 loss: 0.0052 lr: 0.005\n",
            "iteration: 6200 loss: 0.0055 lr: 0.005\n",
            "iteration: 6210 loss: 0.0067 lr: 0.005\n",
            "iteration: 6220 loss: 0.0042 lr: 0.005\n",
            "iteration: 6230 loss: 0.0047 lr: 0.005\n",
            "iteration: 6240 loss: 0.0059 lr: 0.005\n",
            "iteration: 6250 loss: 0.0041 lr: 0.005\n",
            "iteration: 6260 loss: 0.0040 lr: 0.005\n",
            "iteration: 6270 loss: 0.0043 lr: 0.005\n",
            "iteration: 6280 loss: 0.0060 lr: 0.005\n",
            "iteration: 6290 loss: 0.0048 lr: 0.005\n",
            "iteration: 6300 loss: 0.0046 lr: 0.005\n",
            "iteration: 6310 loss: 0.0055 lr: 0.005\n",
            "iteration: 6320 loss: 0.0048 lr: 0.005\n",
            "iteration: 6330 loss: 0.0073 lr: 0.005\n",
            "iteration: 6340 loss: 0.0047 lr: 0.005\n",
            "iteration: 6350 loss: 0.0049 lr: 0.005\n",
            "iteration: 6360 loss: 0.0050 lr: 0.005\n",
            "iteration: 6370 loss: 0.0065 lr: 0.005\n",
            "iteration: 6380 loss: 0.0049 lr: 0.005\n",
            "iteration: 6390 loss: 0.0050 lr: 0.005\n",
            "iteration: 6400 loss: 0.0062 lr: 0.005\n",
            "iteration: 6410 loss: 0.0040 lr: 0.005\n",
            "iteration: 6420 loss: 0.0050 lr: 0.005\n",
            "iteration: 6430 loss: 0.0036 lr: 0.005\n",
            "iteration: 6440 loss: 0.0056 lr: 0.005\n",
            "iteration: 6450 loss: 0.0055 lr: 0.005\n",
            "iteration: 6460 loss: 0.0046 lr: 0.005\n",
            "iteration: 6470 loss: 0.0046 lr: 0.005\n",
            "iteration: 6480 loss: 0.0051 lr: 0.005\n",
            "iteration: 6490 loss: 0.0043 lr: 0.005\n",
            "iteration: 6500 loss: 0.0068 lr: 0.005\n",
            "iteration: 6510 loss: 0.0049 lr: 0.005\n",
            "iteration: 6520 loss: 0.0056 lr: 0.005\n",
            "iteration: 6530 loss: 0.0040 lr: 0.005\n",
            "iteration: 6540 loss: 0.0078 lr: 0.005\n",
            "iteration: 6550 loss: 0.0048 lr: 0.005\n",
            "iteration: 6560 loss: 0.0038 lr: 0.005\n",
            "iteration: 6570 loss: 0.0045 lr: 0.005\n",
            "iteration: 6580 loss: 0.0040 lr: 0.005\n",
            "iteration: 6590 loss: 0.0058 lr: 0.005\n",
            "iteration: 6600 loss: 0.0042 lr: 0.005\n",
            "iteration: 6610 loss: 0.0037 lr: 0.005\n",
            "iteration: 6620 loss: 0.0049 lr: 0.005\n",
            "iteration: 6630 loss: 0.0061 lr: 0.005\n",
            "iteration: 6640 loss: 0.0033 lr: 0.005\n",
            "iteration: 6650 loss: 0.0037 lr: 0.005\n",
            "iteration: 6660 loss: 0.0047 lr: 0.005\n",
            "iteration: 6670 loss: 0.0039 lr: 0.005\n",
            "iteration: 6680 loss: 0.0038 lr: 0.005\n",
            "iteration: 6690 loss: 0.0053 lr: 0.005\n",
            "iteration: 6700 loss: 0.0046 lr: 0.005\n",
            "iteration: 6710 loss: 0.0045 lr: 0.005\n",
            "iteration: 6720 loss: 0.0045 lr: 0.005\n",
            "iteration: 6730 loss: 0.0055 lr: 0.005\n",
            "iteration: 6740 loss: 0.0057 lr: 0.005\n",
            "iteration: 6750 loss: 0.0051 lr: 0.005\n",
            "iteration: 6760 loss: 0.0047 lr: 0.005\n",
            "iteration: 6770 loss: 0.0040 lr: 0.005\n",
            "iteration: 6780 loss: 0.0040 lr: 0.005\n",
            "iteration: 6790 loss: 0.0041 lr: 0.005\n",
            "iteration: 6800 loss: 0.0074 lr: 0.005\n",
            "iteration: 6810 loss: 0.0044 lr: 0.005\n",
            "iteration: 6820 loss: 0.0052 lr: 0.005\n",
            "iteration: 6830 loss: 0.0049 lr: 0.005\n",
            "iteration: 6840 loss: 0.0050 lr: 0.005\n",
            "iteration: 6850 loss: 0.0042 lr: 0.005\n",
            "iteration: 6860 loss: 0.0050 lr: 0.005\n",
            "iteration: 6870 loss: 0.0069 lr: 0.005\n",
            "iteration: 6880 loss: 0.0047 lr: 0.005\n",
            "iteration: 6890 loss: 0.0034 lr: 0.005\n",
            "iteration: 6900 loss: 0.0061 lr: 0.005\n",
            "iteration: 6910 loss: 0.0040 lr: 0.005\n",
            "iteration: 6920 loss: 0.0052 lr: 0.005\n",
            "iteration: 6930 loss: 0.0056 lr: 0.005\n",
            "iteration: 6940 loss: 0.0040 lr: 0.005\n",
            "iteration: 6950 loss: 0.0045 lr: 0.005\n",
            "iteration: 6960 loss: 0.0061 lr: 0.005\n",
            "iteration: 6970 loss: 0.0043 lr: 0.005\n",
            "iteration: 6980 loss: 0.0048 lr: 0.005\n",
            "iteration: 6990 loss: 0.0039 lr: 0.005\n",
            "iteration: 7000 loss: 0.0061 lr: 0.005\n",
            "iteration: 7010 loss: 0.0034 lr: 0.005\n",
            "iteration: 7020 loss: 0.0056 lr: 0.005\n",
            "iteration: 7030 loss: 0.0050 lr: 0.005\n",
            "iteration: 7040 loss: 0.0042 lr: 0.005\n",
            "iteration: 7050 loss: 0.0063 lr: 0.005\n",
            "iteration: 7060 loss: 0.0054 lr: 0.005\n",
            "iteration: 7070 loss: 0.0055 lr: 0.005\n",
            "iteration: 7080 loss: 0.0042 lr: 0.005\n",
            "iteration: 7090 loss: 0.0064 lr: 0.005\n",
            "iteration: 7100 loss: 0.0050 lr: 0.005\n",
            "iteration: 7110 loss: 0.0051 lr: 0.005\n",
            "iteration: 7120 loss: 0.0042 lr: 0.005\n",
            "iteration: 7130 loss: 0.0048 lr: 0.005\n",
            "iteration: 7140 loss: 0.0048 lr: 0.005\n",
            "iteration: 7150 loss: 0.0055 lr: 0.005\n",
            "iteration: 7160 loss: 0.0037 lr: 0.005\n",
            "iteration: 7170 loss: 0.0047 lr: 0.005\n",
            "iteration: 7180 loss: 0.0056 lr: 0.005\n",
            "iteration: 7190 loss: 0.0052 lr: 0.005\n",
            "iteration: 7200 loss: 0.0035 lr: 0.005\n",
            "iteration: 7210 loss: 0.0050 lr: 0.005\n",
            "iteration: 7220 loss: 0.0039 lr: 0.005\n",
            "iteration: 7230 loss: 0.0056 lr: 0.005\n",
            "iteration: 7240 loss: 0.0035 lr: 0.005\n",
            "iteration: 7250 loss: 0.0052 lr: 0.005\n",
            "iteration: 7260 loss: 0.0041 lr: 0.005\n",
            "iteration: 7270 loss: 0.0049 lr: 0.005\n",
            "iteration: 7280 loss: 0.0032 lr: 0.005\n",
            "iteration: 7290 loss: 0.0051 lr: 0.005\n",
            "iteration: 7300 loss: 0.0053 lr: 0.005\n",
            "iteration: 7310 loss: 0.0050 lr: 0.005\n",
            "iteration: 7320 loss: 0.0064 lr: 0.005\n",
            "iteration: 7330 loss: 0.0055 lr: 0.005\n",
            "iteration: 7340 loss: 0.0045 lr: 0.005\n",
            "iteration: 7350 loss: 0.0052 lr: 0.005\n",
            "iteration: 7360 loss: 0.0060 lr: 0.005\n",
            "iteration: 7370 loss: 0.0047 lr: 0.005\n",
            "iteration: 7380 loss: 0.0056 lr: 0.005\n",
            "iteration: 7390 loss: 0.0043 lr: 0.005\n",
            "iteration: 7400 loss: 0.0049 lr: 0.005\n",
            "iteration: 7410 loss: 0.0066 lr: 0.005\n",
            "iteration: 7420 loss: 0.0040 lr: 0.005\n",
            "iteration: 7430 loss: 0.0057 lr: 0.005\n",
            "iteration: 7440 loss: 0.0049 lr: 0.005\n",
            "iteration: 7450 loss: 0.0051 lr: 0.005\n",
            "iteration: 7460 loss: 0.0040 lr: 0.005\n",
            "iteration: 7470 loss: 0.0051 lr: 0.005\n",
            "iteration: 7480 loss: 0.0049 lr: 0.005\n",
            "iteration: 7490 loss: 0.0052 lr: 0.005\n",
            "iteration: 7500 loss: 0.0069 lr: 0.005\n",
            "iteration: 7510 loss: 0.0041 lr: 0.005\n",
            "iteration: 7520 loss: 0.0058 lr: 0.005\n",
            "iteration: 7530 loss: 0.0066 lr: 0.005\n",
            "iteration: 7540 loss: 0.0048 lr: 0.005\n",
            "iteration: 7550 loss: 0.0040 lr: 0.005\n",
            "iteration: 7560 loss: 0.0046 lr: 0.005\n",
            "iteration: 7570 loss: 0.0073 lr: 0.005\n",
            "iteration: 7580 loss: 0.0048 lr: 0.005\n",
            "iteration: 7590 loss: 0.0067 lr: 0.005\n",
            "iteration: 7600 loss: 0.0048 lr: 0.005\n",
            "iteration: 7610 loss: 0.0069 lr: 0.005\n",
            "iteration: 7620 loss: 0.0068 lr: 0.005\n",
            "iteration: 7630 loss: 0.0041 lr: 0.005\n",
            "iteration: 7640 loss: 0.0068 lr: 0.005\n",
            "iteration: 7650 loss: 0.0048 lr: 0.005\n",
            "iteration: 7660 loss: 0.0068 lr: 0.005\n",
            "iteration: 7670 loss: 0.0041 lr: 0.005\n",
            "iteration: 7680 loss: 0.0050 lr: 0.005\n",
            "iteration: 7690 loss: 0.0064 lr: 0.005\n",
            "iteration: 7700 loss: 0.0041 lr: 0.005\n",
            "iteration: 7710 loss: 0.0053 lr: 0.005\n",
            "iteration: 7720 loss: 0.0052 lr: 0.005\n",
            "iteration: 7730 loss: 0.0070 lr: 0.005\n",
            "iteration: 7740 loss: 0.0047 lr: 0.005\n",
            "iteration: 7750 loss: 0.0036 lr: 0.005\n",
            "iteration: 7760 loss: 0.0038 lr: 0.005\n",
            "iteration: 7770 loss: 0.0046 lr: 0.005\n",
            "iteration: 7780 loss: 0.0040 lr: 0.005\n",
            "iteration: 7790 loss: 0.0041 lr: 0.005\n",
            "iteration: 7800 loss: 0.0044 lr: 0.005\n",
            "iteration: 7810 loss: 0.0041 lr: 0.005\n",
            "iteration: 7820 loss: 0.0051 lr: 0.005\n",
            "iteration: 7830 loss: 0.0053 lr: 0.005\n",
            "iteration: 7840 loss: 0.0032 lr: 0.005\n",
            "iteration: 7850 loss: 0.0037 lr: 0.005\n",
            "iteration: 7860 loss: 0.0051 lr: 0.005\n",
            "iteration: 7870 loss: 0.0056 lr: 0.005\n",
            "iteration: 7880 loss: 0.0041 lr: 0.005\n",
            "iteration: 7890 loss: 0.0051 lr: 0.005\n",
            "iteration: 7900 loss: 0.0067 lr: 0.005\n",
            "iteration: 7910 loss: 0.0039 lr: 0.005\n",
            "iteration: 7920 loss: 0.0051 lr: 0.005\n",
            "iteration: 7930 loss: 0.0042 lr: 0.005\n",
            "iteration: 7940 loss: 0.0043 lr: 0.005\n",
            "iteration: 7950 loss: 0.0040 lr: 0.005\n",
            "iteration: 7960 loss: 0.0037 lr: 0.005\n",
            "iteration: 7970 loss: 0.0048 lr: 0.005\n",
            "iteration: 7980 loss: 0.0058 lr: 0.005\n",
            "iteration: 7990 loss: 0.0034 lr: 0.005\n",
            "iteration: 8000 loss: 0.0043 lr: 0.005\n",
            "iteration: 8010 loss: 0.0049 lr: 0.005\n",
            "iteration: 8020 loss: 0.0083 lr: 0.005\n",
            "iteration: 8030 loss: 0.0065 lr: 0.005\n",
            "iteration: 8040 loss: 0.0046 lr: 0.005\n",
            "iteration: 8050 loss: 0.0046 lr: 0.005\n",
            "iteration: 8060 loss: 0.0046 lr: 0.005\n",
            "iteration: 8070 loss: 0.0078 lr: 0.005\n",
            "iteration: 8080 loss: 0.0049 lr: 0.005\n",
            "iteration: 8090 loss: 0.0047 lr: 0.005\n",
            "iteration: 8100 loss: 0.0032 lr: 0.005\n",
            "iteration: 8110 loss: 0.0045 lr: 0.005\n",
            "iteration: 8120 loss: 0.0039 lr: 0.005\n",
            "iteration: 8130 loss: 0.0040 lr: 0.005\n",
            "iteration: 8140 loss: 0.0036 lr: 0.005\n",
            "iteration: 8150 loss: 0.0041 lr: 0.005\n",
            "iteration: 8160 loss: 0.0043 lr: 0.005\n",
            "iteration: 8170 loss: 0.0044 lr: 0.005\n",
            "iteration: 8180 loss: 0.0034 lr: 0.005\n",
            "iteration: 8190 loss: 0.0042 lr: 0.005\n",
            "iteration: 8200 loss: 0.0037 lr: 0.005\n",
            "iteration: 8210 loss: 0.0044 lr: 0.005\n",
            "iteration: 8220 loss: 0.0044 lr: 0.005\n",
            "iteration: 8230 loss: 0.0048 lr: 0.005\n",
            "iteration: 8240 loss: 0.0042 lr: 0.005\n",
            "iteration: 8250 loss: 0.0056 lr: 0.005\n",
            "iteration: 8260 loss: 0.0041 lr: 0.005\n",
            "iteration: 8270 loss: 0.0075 lr: 0.005\n",
            "iteration: 8280 loss: 0.0051 lr: 0.005\n",
            "iteration: 8290 loss: 0.0044 lr: 0.005\n",
            "iteration: 8300 loss: 0.0058 lr: 0.005\n",
            "iteration: 8310 loss: 0.0045 lr: 0.005\n",
            "iteration: 8320 loss: 0.0041 lr: 0.005\n",
            "iteration: 8330 loss: 0.0054 lr: 0.005\n",
            "iteration: 8340 loss: 0.0049 lr: 0.005\n",
            "iteration: 8350 loss: 0.0049 lr: 0.005\n",
            "iteration: 8360 loss: 0.0042 lr: 0.005\n",
            "iteration: 8370 loss: 0.0064 lr: 0.005\n",
            "iteration: 8380 loss: 0.0051 lr: 0.005\n",
            "iteration: 8390 loss: 0.0045 lr: 0.005\n",
            "iteration: 8400 loss: 0.0063 lr: 0.005\n",
            "iteration: 8410 loss: 0.0048 lr: 0.005\n",
            "iteration: 8420 loss: 0.0068 lr: 0.005\n",
            "iteration: 8430 loss: 0.0043 lr: 0.005\n",
            "iteration: 8440 loss: 0.0045 lr: 0.005\n",
            "iteration: 8450 loss: 0.0049 lr: 0.005\n",
            "iteration: 8460 loss: 0.0037 lr: 0.005\n",
            "iteration: 8470 loss: 0.0048 lr: 0.005\n",
            "iteration: 8480 loss: 0.0045 lr: 0.005\n",
            "iteration: 8490 loss: 0.0041 lr: 0.005\n",
            "iteration: 8500 loss: 0.0070 lr: 0.005\n",
            "iteration: 8510 loss: 0.0041 lr: 0.005\n",
            "iteration: 8520 loss: 0.0042 lr: 0.005\n",
            "iteration: 8530 loss: 0.0050 lr: 0.005\n",
            "iteration: 8540 loss: 0.0036 lr: 0.005\n",
            "iteration: 8550 loss: 0.0037 lr: 0.005\n",
            "iteration: 8560 loss: 0.0056 lr: 0.005\n",
            "iteration: 8570 loss: 0.0032 lr: 0.005\n",
            "iteration: 8580 loss: 0.0053 lr: 0.005\n",
            "iteration: 8590 loss: 0.0053 lr: 0.005\n",
            "iteration: 8600 loss: 0.0042 lr: 0.005\n",
            "iteration: 8610 loss: 0.0035 lr: 0.005\n",
            "iteration: 8620 loss: 0.0041 lr: 0.005\n",
            "iteration: 8630 loss: 0.0046 lr: 0.005\n",
            "iteration: 8640 loss: 0.0040 lr: 0.005\n",
            "iteration: 8650 loss: 0.0051 lr: 0.005\n",
            "iteration: 8660 loss: 0.0034 lr: 0.005\n",
            "iteration: 8670 loss: 0.0040 lr: 0.005\n",
            "iteration: 8680 loss: 0.0049 lr: 0.005\n",
            "iteration: 8690 loss: 0.0057 lr: 0.005\n",
            "iteration: 8700 loss: 0.0055 lr: 0.005\n",
            "iteration: 8710 loss: 0.0053 lr: 0.005\n",
            "iteration: 8720 loss: 0.0043 lr: 0.005\n",
            "iteration: 8730 loss: 0.0052 lr: 0.005\n",
            "iteration: 8740 loss: 0.0036 lr: 0.005\n",
            "iteration: 8750 loss: 0.0040 lr: 0.005\n",
            "iteration: 8760 loss: 0.0029 lr: 0.005\n",
            "iteration: 8770 loss: 0.0040 lr: 0.005\n",
            "iteration: 8780 loss: 0.0036 lr: 0.005\n",
            "iteration: 8790 loss: 0.0040 lr: 0.005\n",
            "iteration: 8800 loss: 0.0032 lr: 0.005\n",
            "iteration: 8810 loss: 0.0041 lr: 0.005\n",
            "iteration: 8820 loss: 0.0064 lr: 0.005\n",
            "iteration: 8830 loss: 0.0048 lr: 0.005\n",
            "iteration: 8840 loss: 0.0036 lr: 0.005\n",
            "iteration: 8850 loss: 0.0041 lr: 0.005\n",
            "iteration: 8860 loss: 0.0037 lr: 0.005\n",
            "iteration: 8870 loss: 0.0036 lr: 0.005\n",
            "iteration: 8880 loss: 0.0033 lr: 0.005\n",
            "iteration: 8890 loss: 0.0040 lr: 0.005\n",
            "iteration: 8900 loss: 0.0041 lr: 0.005\n",
            "iteration: 8910 loss: 0.0042 lr: 0.005\n",
            "iteration: 8920 loss: 0.0032 lr: 0.005\n",
            "iteration: 8930 loss: 0.0037 lr: 0.005\n",
            "iteration: 8940 loss: 0.0044 lr: 0.005\n",
            "iteration: 8950 loss: 0.0048 lr: 0.005\n",
            "iteration: 8960 loss: 0.0040 lr: 0.005\n",
            "iteration: 8970 loss: 0.0049 lr: 0.005\n",
            "iteration: 8980 loss: 0.0032 lr: 0.005\n",
            "iteration: 8990 loss: 0.0047 lr: 0.005\n",
            "iteration: 9000 loss: 0.0062 lr: 0.005\n",
            "iteration: 9010 loss: 0.0032 lr: 0.005\n",
            "iteration: 9020 loss: 0.0032 lr: 0.005\n",
            "iteration: 9030 loss: 0.0047 lr: 0.005\n",
            "iteration: 9040 loss: 0.0052 lr: 0.005\n",
            "iteration: 9050 loss: 0.0044 lr: 0.005\n",
            "iteration: 9060 loss: 0.0040 lr: 0.005\n",
            "iteration: 9070 loss: 0.0037 lr: 0.005\n",
            "iteration: 9080 loss: 0.0035 lr: 0.005\n",
            "iteration: 9090 loss: 0.0053 lr: 0.005\n",
            "iteration: 9100 loss: 0.0043 lr: 0.005\n",
            "iteration: 9110 loss: 0.0054 lr: 0.005\n",
            "iteration: 9120 loss: 0.0038 lr: 0.005\n",
            "iteration: 9130 loss: 0.0088 lr: 0.005\n",
            "iteration: 9140 loss: 0.0052 lr: 0.005\n",
            "iteration: 9150 loss: 0.0047 lr: 0.005\n",
            "iteration: 9160 loss: 0.0057 lr: 0.005\n",
            "iteration: 9170 loss: 0.0052 lr: 0.005\n",
            "iteration: 9180 loss: 0.0051 lr: 0.005\n",
            "iteration: 9190 loss: 0.0036 lr: 0.005\n",
            "iteration: 9200 loss: 0.0050 lr: 0.005\n",
            "iteration: 9210 loss: 0.0036 lr: 0.005\n",
            "iteration: 9220 loss: 0.0040 lr: 0.005\n",
            "iteration: 9230 loss: 0.0029 lr: 0.005\n",
            "iteration: 9240 loss: 0.0032 lr: 0.005\n",
            "iteration: 9250 loss: 0.0043 lr: 0.005\n",
            "iteration: 9260 loss: 0.0032 lr: 0.005\n",
            "iteration: 9270 loss: 0.0036 lr: 0.005\n",
            "iteration: 9280 loss: 0.0049 lr: 0.005\n",
            "iteration: 9290 loss: 0.0043 lr: 0.005\n",
            "iteration: 9300 loss: 0.0042 lr: 0.005\n",
            "iteration: 9310 loss: 0.0047 lr: 0.005\n",
            "iteration: 9320 loss: 0.0041 lr: 0.005\n",
            "iteration: 9330 loss: 0.0043 lr: 0.005\n",
            "iteration: 9340 loss: 0.0059 lr: 0.005\n",
            "iteration: 9350 loss: 0.0041 lr: 0.005\n",
            "iteration: 9360 loss: 0.0060 lr: 0.005\n",
            "iteration: 9370 loss: 0.0043 lr: 0.005\n",
            "iteration: 9380 loss: 0.0037 lr: 0.005\n",
            "iteration: 9390 loss: 0.0044 lr: 0.005\n",
            "iteration: 9400 loss: 0.0038 lr: 0.005\n",
            "iteration: 9410 loss: 0.0052 lr: 0.005\n",
            "iteration: 9420 loss: 0.0034 lr: 0.005\n",
            "iteration: 9430 loss: 0.0045 lr: 0.005\n",
            "iteration: 9440 loss: 0.0043 lr: 0.005\n",
            "iteration: 9450 loss: 0.0054 lr: 0.005\n",
            "iteration: 9460 loss: 0.0031 lr: 0.005\n",
            "iteration: 9470 loss: 0.0037 lr: 0.005\n",
            "iteration: 9480 loss: 0.0038 lr: 0.005\n",
            "iteration: 9490 loss: 0.0044 lr: 0.005\n",
            "iteration: 9500 loss: 0.0045 lr: 0.005\n",
            "iteration: 9510 loss: 0.0037 lr: 0.005\n",
            "iteration: 9520 loss: 0.0035 lr: 0.005\n",
            "iteration: 9530 loss: 0.0049 lr: 0.005\n",
            "iteration: 9540 loss: 0.0053 lr: 0.005\n",
            "iteration: 9550 loss: 0.0048 lr: 0.005\n",
            "iteration: 9560 loss: 0.0033 lr: 0.005\n",
            "iteration: 9570 loss: 0.0041 lr: 0.005\n",
            "iteration: 9580 loss: 0.0027 lr: 0.005\n",
            "iteration: 9590 loss: 0.0036 lr: 0.005\n",
            "iteration: 9600 loss: 0.0064 lr: 0.005\n",
            "iteration: 9610 loss: 0.0034 lr: 0.005\n",
            "iteration: 9620 loss: 0.0056 lr: 0.005\n",
            "iteration: 9630 loss: 0.0039 lr: 0.005\n",
            "iteration: 9640 loss: 0.0046 lr: 0.005\n",
            "iteration: 9650 loss: 0.0037 lr: 0.005\n",
            "iteration: 9660 loss: 0.0034 lr: 0.005\n",
            "iteration: 9670 loss: 0.0046 lr: 0.005\n",
            "iteration: 9680 loss: 0.0040 lr: 0.005\n",
            "iteration: 9690 loss: 0.0049 lr: 0.005\n",
            "iteration: 9700 loss: 0.0038 lr: 0.005\n",
            "iteration: 9710 loss: 0.0041 lr: 0.005\n",
            "iteration: 9720 loss: 0.0040 lr: 0.005\n",
            "iteration: 9730 loss: 0.0044 lr: 0.005\n",
            "iteration: 9740 loss: 0.0028 lr: 0.005\n",
            "iteration: 9750 loss: 0.0028 lr: 0.005\n",
            "iteration: 9760 loss: 0.0039 lr: 0.005\n",
            "iteration: 9770 loss: 0.0034 lr: 0.005\n",
            "iteration: 9780 loss: 0.0037 lr: 0.005\n",
            "iteration: 9790 loss: 0.0041 lr: 0.005\n",
            "iteration: 9800 loss: 0.0038 lr: 0.005\n",
            "iteration: 9810 loss: 0.0035 lr: 0.005\n",
            "iteration: 9820 loss: 0.0036 lr: 0.005\n",
            "iteration: 9830 loss: 0.0048 lr: 0.005\n",
            "iteration: 9840 loss: 0.0064 lr: 0.005\n",
            "iteration: 9850 loss: 0.0032 lr: 0.005\n",
            "iteration: 9860 loss: 0.0034 lr: 0.005\n",
            "iteration: 9870 loss: 0.0034 lr: 0.005\n",
            "iteration: 9880 loss: 0.0041 lr: 0.005\n",
            "iteration: 9890 loss: 0.0051 lr: 0.005\n",
            "iteration: 9900 loss: 0.0047 lr: 0.005\n",
            "iteration: 9910 loss: 0.0042 lr: 0.005\n",
            "iteration: 9920 loss: 0.0035 lr: 0.005\n",
            "iteration: 9930 loss: 0.0039 lr: 0.005\n",
            "iteration: 9940 loss: 0.0035 lr: 0.005\n",
            "iteration: 9950 loss: 0.0030 lr: 0.005\n",
            "iteration: 9960 loss: 0.0038 lr: 0.005\n",
            "iteration: 9970 loss: 0.0062 lr: 0.005\n",
            "iteration: 9980 loss: 0.0037 lr: 0.005\n",
            "iteration: 9990 loss: 0.0052 lr: 0.005\n",
            "iteration: 10000 loss: 0.0044 lr: 0.005\n",
            "iteration: 10010 loss: 0.0058 lr: 0.02\n",
            "iteration: 10020 loss: 0.0056 lr: 0.02\n",
            "iteration: 10030 loss: 0.0064 lr: 0.02\n",
            "iteration: 10040 loss: 0.0057 lr: 0.02\n",
            "iteration: 10050 loss: 0.0075 lr: 0.02\n",
            "iteration: 10060 loss: 0.0066 lr: 0.02\n",
            "iteration: 10070 loss: 0.0070 lr: 0.02\n",
            "iteration: 10080 loss: 0.0075 lr: 0.02\n",
            "iteration: 10090 loss: 0.0047 lr: 0.02\n",
            "iteration: 10100 loss: 0.0059 lr: 0.02\n",
            "iteration: 10110 loss: 0.0043 lr: 0.02\n",
            "iteration: 10120 loss: 0.0067 lr: 0.02\n",
            "iteration: 10130 loss: 0.0079 lr: 0.02\n",
            "iteration: 10140 loss: 0.0047 lr: 0.02\n",
            "iteration: 10150 loss: 0.0052 lr: 0.02\n",
            "iteration: 10160 loss: 0.0074 lr: 0.02\n",
            "iteration: 10170 loss: 0.0047 lr: 0.02\n",
            "iteration: 10180 loss: 0.0090 lr: 0.02\n",
            "iteration: 10190 loss: 0.0051 lr: 0.02\n",
            "iteration: 10200 loss: 0.0054 lr: 0.02\n",
            "iteration: 10210 loss: 0.0041 lr: 0.02\n",
            "iteration: 10220 loss: 0.0060 lr: 0.02\n",
            "iteration: 10230 loss: 0.0070 lr: 0.02\n",
            "iteration: 10240 loss: 0.0068 lr: 0.02\n",
            "iteration: 10250 loss: 0.0066 lr: 0.02\n",
            "iteration: 10260 loss: 0.0063 lr: 0.02\n",
            "iteration: 10270 loss: 0.0072 lr: 0.02\n",
            "iteration: 10280 loss: 0.0045 lr: 0.02\n",
            "iteration: 10290 loss: 0.0089 lr: 0.02\n",
            "iteration: 10300 loss: 0.0066 lr: 0.02\n",
            "iteration: 10310 loss: 0.0055 lr: 0.02\n",
            "iteration: 10320 loss: 0.0079 lr: 0.02\n",
            "iteration: 10330 loss: 0.0056 lr: 0.02\n",
            "iteration: 10340 loss: 0.0071 lr: 0.02\n",
            "iteration: 10350 loss: 0.0065 lr: 0.02\n",
            "iteration: 10360 loss: 0.0061 lr: 0.02\n",
            "iteration: 10370 loss: 0.0062 lr: 0.02\n",
            "iteration: 10380 loss: 0.0066 lr: 0.02\n",
            "iteration: 10390 loss: 0.0103 lr: 0.02\n",
            "iteration: 10400 loss: 0.0070 lr: 0.02\n",
            "iteration: 10410 loss: 0.0080 lr: 0.02\n",
            "iteration: 10420 loss: 0.0070 lr: 0.02\n",
            "iteration: 10430 loss: 0.0068 lr: 0.02\n",
            "iteration: 10440 loss: 0.0082 lr: 0.02\n",
            "iteration: 10450 loss: 0.0068 lr: 0.02\n",
            "iteration: 10460 loss: 0.0054 lr: 0.02\n",
            "iteration: 10470 loss: 0.0045 lr: 0.02\n",
            "iteration: 10480 loss: 0.0047 lr: 0.02\n",
            "iteration: 10490 loss: 0.0048 lr: 0.02\n",
            "iteration: 10500 loss: 0.0050 lr: 0.02\n",
            "iteration: 10510 loss: 0.0052 lr: 0.02\n",
            "iteration: 10520 loss: 0.0062 lr: 0.02\n",
            "iteration: 10530 loss: 0.0072 lr: 0.02\n",
            "iteration: 10540 loss: 0.0055 lr: 0.02\n",
            "iteration: 10550 loss: 0.0052 lr: 0.02\n",
            "iteration: 10560 loss: 0.0059 lr: 0.02\n",
            "iteration: 10570 loss: 0.0064 lr: 0.02\n",
            "iteration: 10580 loss: 0.0062 lr: 0.02\n",
            "iteration: 10590 loss: 0.0052 lr: 0.02\n",
            "iteration: 10600 loss: 0.0055 lr: 0.02\n",
            "iteration: 10610 loss: 0.0056 lr: 0.02\n",
            "iteration: 10620 loss: 0.0082 lr: 0.02\n",
            "iteration: 10630 loss: 0.0080 lr: 0.02\n",
            "iteration: 10640 loss: 0.0068 lr: 0.02\n",
            "iteration: 10650 loss: 0.0065 lr: 0.02\n",
            "iteration: 10660 loss: 0.0080 lr: 0.02\n",
            "iteration: 10670 loss: 0.0099 lr: 0.02\n",
            "iteration: 10680 loss: 0.0087 lr: 0.02\n",
            "iteration: 10690 loss: 0.0068 lr: 0.02\n",
            "iteration: 10700 loss: 0.0051 lr: 0.02\n",
            "iteration: 10710 loss: 0.0049 lr: 0.02\n",
            "iteration: 10720 loss: 0.0060 lr: 0.02\n",
            "iteration: 10730 loss: 0.0063 lr: 0.02\n",
            "iteration: 10740 loss: 0.0042 lr: 0.02\n",
            "iteration: 10750 loss: 0.0071 lr: 0.02\n",
            "iteration: 10760 loss: 0.0055 lr: 0.02\n",
            "iteration: 10770 loss: 0.0055 lr: 0.02\n",
            "iteration: 10780 loss: 0.0082 lr: 0.02\n",
            "iteration: 10790 loss: 0.0151 lr: 0.02\n",
            "iteration: 10800 loss: 0.0122 lr: 0.02\n",
            "iteration: 10810 loss: 0.0073 lr: 0.02\n",
            "iteration: 10820 loss: 0.0056 lr: 0.02\n",
            "iteration: 10830 loss: 0.0079 lr: 0.02\n",
            "iteration: 10840 loss: 0.0053 lr: 0.02\n",
            "iteration: 10850 loss: 0.0087 lr: 0.02\n",
            "iteration: 10860 loss: 0.0060 lr: 0.02\n",
            "iteration: 10870 loss: 0.0051 lr: 0.02\n",
            "iteration: 10880 loss: 0.0056 lr: 0.02\n",
            "iteration: 10890 loss: 0.0070 lr: 0.02\n",
            "iteration: 10900 loss: 0.0080 lr: 0.02\n",
            "iteration: 10910 loss: 0.0049 lr: 0.02\n",
            "iteration: 10920 loss: 0.0051 lr: 0.02\n",
            "iteration: 10930 loss: 0.0048 lr: 0.02\n",
            "iteration: 10940 loss: 0.0048 lr: 0.02\n",
            "iteration: 10950 loss: 0.0051 lr: 0.02\n",
            "iteration: 10960 loss: 0.0058 lr: 0.02\n",
            "iteration: 10970 loss: 0.0054 lr: 0.02\n",
            "iteration: 10980 loss: 0.0082 lr: 0.02\n",
            "iteration: 10990 loss: 0.0046 lr: 0.02\n",
            "iteration: 11000 loss: 0.0046 lr: 0.02\n",
            "iteration: 11010 loss: 0.0112 lr: 0.02\n",
            "iteration: 11020 loss: 0.0073 lr: 0.02\n",
            "iteration: 11030 loss: 0.0052 lr: 0.02\n",
            "iteration: 11040 loss: 0.0057 lr: 0.02\n",
            "iteration: 11050 loss: 0.0069 lr: 0.02\n",
            "iteration: 11060 loss: 0.0054 lr: 0.02\n",
            "iteration: 11070 loss: 0.0050 lr: 0.02\n",
            "iteration: 11080 loss: 0.0047 lr: 0.02\n",
            "iteration: 11090 loss: 0.0068 lr: 0.02\n",
            "iteration: 11100 loss: 0.0065 lr: 0.02\n",
            "iteration: 11110 loss: 0.0078 lr: 0.02\n",
            "iteration: 11120 loss: 0.0060 lr: 0.02\n",
            "iteration: 11130 loss: 0.0059 lr: 0.02\n",
            "iteration: 11140 loss: 0.0057 lr: 0.02\n",
            "iteration: 11150 loss: 0.0058 lr: 0.02\n",
            "iteration: 11160 loss: 0.0047 lr: 0.02\n",
            "iteration: 11170 loss: 0.0055 lr: 0.02\n",
            "iteration: 11180 loss: 0.0049 lr: 0.02\n",
            "iteration: 11190 loss: 0.0051 lr: 0.02\n",
            "iteration: 11200 loss: 0.0057 lr: 0.02\n",
            "iteration: 11210 loss: 0.0052 lr: 0.02\n",
            "iteration: 11220 loss: 0.0054 lr: 0.02\n",
            "iteration: 11230 loss: 0.0050 lr: 0.02\n",
            "iteration: 11240 loss: 0.0040 lr: 0.02\n",
            "iteration: 11250 loss: 0.0040 lr: 0.02\n",
            "iteration: 11260 loss: 0.0039 lr: 0.02\n",
            "iteration: 11270 loss: 0.0046 lr: 0.02\n",
            "iteration: 11280 loss: 0.0045 lr: 0.02\n",
            "iteration: 11290 loss: 0.0044 lr: 0.02\n",
            "iteration: 11300 loss: 0.0057 lr: 0.02\n",
            "iteration: 11310 loss: 0.0056 lr: 0.02\n",
            "iteration: 11320 loss: 0.0048 lr: 0.02\n",
            "iteration: 11330 loss: 0.0046 lr: 0.02\n",
            "iteration: 11340 loss: 0.0072 lr: 0.02\n",
            "iteration: 11350 loss: 0.0055 lr: 0.02\n",
            "iteration: 11360 loss: 0.0067 lr: 0.02\n",
            "iteration: 11370 loss: 0.0054 lr: 0.02\n",
            "iteration: 11380 loss: 0.0049 lr: 0.02\n",
            "iteration: 11390 loss: 0.0060 lr: 0.02\n",
            "iteration: 11400 loss: 0.0059 lr: 0.02\n",
            "iteration: 11410 loss: 0.0057 lr: 0.02\n",
            "iteration: 11420 loss: 0.0067 lr: 0.02\n",
            "iteration: 11430 loss: 0.0057 lr: 0.02\n",
            "iteration: 11440 loss: 0.0045 lr: 0.02\n",
            "iteration: 11450 loss: 0.0059 lr: 0.02\n",
            "iteration: 11460 loss: 0.0064 lr: 0.02\n",
            "iteration: 11470 loss: 0.0051 lr: 0.02\n",
            "iteration: 11480 loss: 0.0047 lr: 0.02\n",
            "iteration: 11490 loss: 0.0061 lr: 0.02\n",
            "iteration: 11500 loss: 0.0050 lr: 0.02\n",
            "iteration: 11510 loss: 0.0040 lr: 0.02\n",
            "iteration: 11520 loss: 0.0059 lr: 0.02\n",
            "iteration: 11530 loss: 0.0056 lr: 0.02\n",
            "iteration: 11540 loss: 0.0054 lr: 0.02\n",
            "iteration: 11550 loss: 0.0057 lr: 0.02\n",
            "iteration: 11560 loss: 0.0044 lr: 0.02\n",
            "iteration: 11570 loss: 0.0051 lr: 0.02\n",
            "iteration: 11580 loss: 0.0053 lr: 0.02\n",
            "iteration: 11590 loss: 0.0077 lr: 0.02\n",
            "iteration: 11600 loss: 0.0041 lr: 0.02\n",
            "iteration: 11610 loss: 0.0047 lr: 0.02\n",
            "iteration: 11620 loss: 0.0040 lr: 0.02\n",
            "iteration: 11630 loss: 0.0063 lr: 0.02\n",
            "iteration: 11640 loss: 0.0057 lr: 0.02\n",
            "iteration: 11650 loss: 0.0046 lr: 0.02\n",
            "iteration: 11660 loss: 0.0066 lr: 0.02\n",
            "iteration: 11670 loss: 0.0057 lr: 0.02\n",
            "iteration: 11680 loss: 0.0042 lr: 0.02\n",
            "iteration: 11690 loss: 0.0047 lr: 0.02\n",
            "iteration: 11700 loss: 0.0041 lr: 0.02\n",
            "iteration: 11710 loss: 0.0044 lr: 0.02\n",
            "iteration: 11720 loss: 0.0035 lr: 0.02\n",
            "iteration: 11730 loss: 0.0040 lr: 0.02\n",
            "iteration: 11740 loss: 0.0041 lr: 0.02\n",
            "iteration: 11750 loss: 0.0049 lr: 0.02\n",
            "iteration: 11760 loss: 0.0053 lr: 0.02\n",
            "iteration: 11770 loss: 0.0059 lr: 0.02\n",
            "iteration: 11780 loss: 0.0050 lr: 0.02\n",
            "iteration: 11790 loss: 0.0050 lr: 0.02\n",
            "iteration: 11800 loss: 0.0040 lr: 0.02\n",
            "iteration: 11810 loss: 0.0039 lr: 0.02\n",
            "iteration: 11820 loss: 0.0045 lr: 0.02\n",
            "iteration: 11830 loss: 0.0054 lr: 0.02\n",
            "iteration: 11840 loss: 0.0051 lr: 0.02\n",
            "iteration: 11850 loss: 0.0030 lr: 0.02\n",
            "iteration: 11860 loss: 0.0035 lr: 0.02\n",
            "iteration: 11870 loss: 0.0043 lr: 0.02\n",
            "iteration: 11880 loss: 0.0046 lr: 0.02\n",
            "iteration: 11890 loss: 0.0043 lr: 0.02\n",
            "iteration: 11900 loss: 0.0051 lr: 0.02\n",
            "iteration: 11910 loss: 0.0048 lr: 0.02\n",
            "iteration: 11920 loss: 0.0039 lr: 0.02\n",
            "iteration: 11930 loss: 0.0045 lr: 0.02\n",
            "iteration: 11940 loss: 0.0041 lr: 0.02\n",
            "iteration: 11950 loss: 0.0043 lr: 0.02\n",
            "iteration: 11960 loss: 0.0046 lr: 0.02\n",
            "iteration: 11970 loss: 0.0044 lr: 0.02\n",
            "iteration: 11980 loss: 0.0032 lr: 0.02\n",
            "iteration: 11990 loss: 0.0030 lr: 0.02\n",
            "iteration: 12000 loss: 0.0066 lr: 0.02\n",
            "iteration: 12010 loss: 0.0048 lr: 0.02\n",
            "iteration: 12020 loss: 0.0044 lr: 0.02\n",
            "iteration: 12030 loss: 0.0041 lr: 0.02\n",
            "iteration: 12040 loss: 0.0031 lr: 0.02\n",
            "iteration: 12050 loss: 0.0041 lr: 0.02\n",
            "iteration: 12060 loss: 0.0034 lr: 0.02\n",
            "iteration: 12070 loss: 0.0037 lr: 0.02\n",
            "iteration: 12080 loss: 0.0082 lr: 0.02\n",
            "iteration: 12090 loss: 0.0074 lr: 0.02\n",
            "iteration: 12100 loss: 0.0075 lr: 0.02\n",
            "iteration: 12110 loss: 0.0073 lr: 0.02\n",
            "iteration: 12120 loss: 0.0052 lr: 0.02\n",
            "iteration: 12130 loss: 0.0051 lr: 0.02\n",
            "iteration: 12140 loss: 0.0037 lr: 0.02\n",
            "iteration: 12150 loss: 0.0052 lr: 0.02\n",
            "iteration: 12160 loss: 0.0047 lr: 0.02\n",
            "iteration: 12170 loss: 0.0045 lr: 0.02\n",
            "iteration: 12180 loss: 0.0040 lr: 0.02\n",
            "iteration: 12190 loss: 0.0049 lr: 0.02\n",
            "iteration: 12200 loss: 0.0065 lr: 0.02\n",
            "iteration: 12210 loss: 0.0053 lr: 0.02\n",
            "iteration: 12220 loss: 0.0065 lr: 0.02\n",
            "iteration: 12230 loss: 0.0059 lr: 0.02\n",
            "iteration: 12240 loss: 0.0057 lr: 0.02\n",
            "iteration: 12250 loss: 0.0072 lr: 0.02\n",
            "iteration: 12260 loss: 0.0053 lr: 0.02\n",
            "iteration: 12270 loss: 0.0046 lr: 0.02\n",
            "iteration: 12280 loss: 0.0037 lr: 0.02\n",
            "iteration: 12290 loss: 0.0060 lr: 0.02\n",
            "iteration: 12300 loss: 0.0041 lr: 0.02\n",
            "iteration: 12310 loss: 0.0044 lr: 0.02\n",
            "iteration: 12320 loss: 0.0047 lr: 0.02\n",
            "iteration: 12330 loss: 0.0036 lr: 0.02\n",
            "iteration: 12340 loss: 0.0043 lr: 0.02\n",
            "iteration: 12350 loss: 0.0049 lr: 0.02\n",
            "iteration: 12360 loss: 0.0049 lr: 0.02\n",
            "iteration: 12370 loss: 0.0047 lr: 0.02\n",
            "iteration: 12380 loss: 0.0063 lr: 0.02\n",
            "iteration: 12390 loss: 0.0045 lr: 0.02\n",
            "iteration: 12400 loss: 0.0039 lr: 0.02\n",
            "iteration: 12410 loss: 0.0034 lr: 0.02\n",
            "iteration: 12420 loss: 0.0032 lr: 0.02\n",
            "iteration: 12430 loss: 0.0046 lr: 0.02\n",
            "iteration: 12440 loss: 0.0060 lr: 0.02\n",
            "iteration: 12450 loss: 0.0034 lr: 0.02\n",
            "iteration: 12460 loss: 0.0040 lr: 0.02\n",
            "iteration: 12470 loss: 0.0049 lr: 0.02\n",
            "iteration: 12480 loss: 0.0063 lr: 0.02\n",
            "iteration: 12490 loss: 0.0110 lr: 0.02\n",
            "iteration: 12500 loss: 0.0078 lr: 0.02\n",
            "iteration: 12510 loss: 0.0070 lr: 0.02\n",
            "iteration: 12520 loss: 0.0040 lr: 0.02\n",
            "iteration: 12530 loss: 0.0051 lr: 0.02\n",
            "iteration: 12540 loss: 0.0053 lr: 0.02\n",
            "iteration: 12550 loss: 0.0047 lr: 0.02\n",
            "iteration: 12560 loss: 0.0042 lr: 0.02\n",
            "iteration: 12570 loss: 0.0043 lr: 0.02\n",
            "iteration: 12580 loss: 0.0053 lr: 0.02\n",
            "iteration: 12590 loss: 0.0042 lr: 0.02\n",
            "iteration: 12600 loss: 0.0046 lr: 0.02\n",
            "iteration: 12610 loss: 0.0040 lr: 0.02\n",
            "iteration: 12620 loss: 0.0034 lr: 0.02\n",
            "iteration: 12630 loss: 0.0032 lr: 0.02\n",
            "iteration: 12640 loss: 0.0058 lr: 0.02\n",
            "iteration: 12650 loss: 0.0058 lr: 0.02\n",
            "iteration: 12660 loss: 0.0058 lr: 0.02\n",
            "iteration: 12670 loss: 0.0044 lr: 0.02\n",
            "iteration: 12680 loss: 0.0043 lr: 0.02\n",
            "iteration: 12690 loss: 0.0040 lr: 0.02\n",
            "iteration: 12700 loss: 0.0049 lr: 0.02\n",
            "iteration: 12710 loss: 0.0047 lr: 0.02\n",
            "iteration: 12720 loss: 0.0055 lr: 0.02\n",
            "iteration: 12730 loss: 0.0041 lr: 0.02\n",
            "iteration: 12740 loss: 0.0061 lr: 0.02\n",
            "iteration: 12750 loss: 0.0034 lr: 0.02\n",
            "iteration: 12760 loss: 0.0046 lr: 0.02\n",
            "iteration: 12770 loss: 0.0038 lr: 0.02\n",
            "iteration: 12780 loss: 0.0042 lr: 0.02\n",
            "iteration: 12790 loss: 0.0035 lr: 0.02\n",
            "iteration: 12800 loss: 0.0040 lr: 0.02\n",
            "iteration: 12810 loss: 0.0038 lr: 0.02\n",
            "iteration: 12820 loss: 0.0039 lr: 0.02\n",
            "iteration: 12830 loss: 0.0047 lr: 0.02\n",
            "iteration: 12840 loss: 0.0053 lr: 0.02\n",
            "iteration: 12850 loss: 0.0037 lr: 0.02\n",
            "iteration: 12860 loss: 0.0034 lr: 0.02\n",
            "iteration: 12870 loss: 0.0050 lr: 0.02\n",
            "iteration: 12880 loss: 0.0036 lr: 0.02\n",
            "iteration: 12890 loss: 0.0046 lr: 0.02\n",
            "iteration: 12900 loss: 0.0048 lr: 0.02\n",
            "iteration: 12910 loss: 0.0038 lr: 0.02\n",
            "iteration: 12920 loss: 0.0034 lr: 0.02\n",
            "iteration: 12930 loss: 0.0049 lr: 0.02\n",
            "iteration: 12940 loss: 0.0076 lr: 0.02\n",
            "iteration: 12950 loss: 0.0045 lr: 0.02\n",
            "iteration: 12960 loss: 0.0049 lr: 0.02\n",
            "iteration: 12970 loss: 0.0045 lr: 0.02\n",
            "iteration: 12980 loss: 0.0041 lr: 0.02\n",
            "iteration: 12990 loss: 0.0042 lr: 0.02\n",
            "iteration: 13000 loss: 0.0043 lr: 0.02\n",
            "iteration: 13010 loss: 0.0055 lr: 0.02\n",
            "iteration: 13020 loss: 0.0036 lr: 0.02\n",
            "iteration: 13030 loss: 0.0045 lr: 0.02\n",
            "iteration: 13040 loss: 0.0037 lr: 0.02\n",
            "iteration: 13050 loss: 0.0045 lr: 0.02\n",
            "iteration: 13060 loss: 0.0042 lr: 0.02\n",
            "iteration: 13070 loss: 0.0042 lr: 0.02\n",
            "iteration: 13080 loss: 0.0036 lr: 0.02\n",
            "iteration: 13090 loss: 0.0034 lr: 0.02\n",
            "iteration: 13100 loss: 0.0037 lr: 0.02\n",
            "iteration: 13110 loss: 0.0063 lr: 0.02\n",
            "iteration: 13120 loss: 0.0053 lr: 0.02\n",
            "iteration: 13130 loss: 0.0040 lr: 0.02\n",
            "iteration: 13140 loss: 0.0054 lr: 0.02\n",
            "iteration: 13150 loss: 0.0038 lr: 0.02\n",
            "iteration: 13160 loss: 0.0039 lr: 0.02\n",
            "iteration: 13170 loss: 0.0032 lr: 0.02\n",
            "iteration: 13180 loss: 0.0039 lr: 0.02\n",
            "iteration: 13190 loss: 0.0029 lr: 0.02\n",
            "iteration: 13200 loss: 0.0049 lr: 0.02\n",
            "iteration: 13210 loss: 0.0048 lr: 0.02\n",
            "iteration: 13220 loss: 0.0043 lr: 0.02\n",
            "iteration: 13230 loss: 0.0045 lr: 0.02\n",
            "iteration: 13240 loss: 0.0042 lr: 0.02\n",
            "iteration: 13250 loss: 0.0042 lr: 0.02\n",
            "iteration: 13260 loss: 0.0042 lr: 0.02\n",
            "iteration: 13270 loss: 0.0049 lr: 0.02\n",
            "iteration: 13280 loss: 0.0046 lr: 0.02\n",
            "iteration: 13290 loss: 0.0050 lr: 0.02\n",
            "iteration: 13300 loss: 0.0039 lr: 0.02\n",
            "iteration: 13310 loss: 0.0044 lr: 0.02\n",
            "iteration: 13320 loss: 0.0035 lr: 0.02\n",
            "iteration: 13330 loss: 0.0050 lr: 0.02\n",
            "iteration: 13340 loss: 0.0047 lr: 0.02\n",
            "iteration: 13350 loss: 0.0068 lr: 0.02\n",
            "iteration: 13360 loss: 0.0048 lr: 0.02\n",
            "iteration: 13370 loss: 0.0046 lr: 0.02\n",
            "iteration: 13380 loss: 0.0040 lr: 0.02\n",
            "iteration: 13390 loss: 0.0034 lr: 0.02\n",
            "iteration: 13400 loss: 0.0037 lr: 0.02\n",
            "iteration: 13410 loss: 0.0040 lr: 0.02\n",
            "iteration: 13420 loss: 0.0056 lr: 0.02\n",
            "iteration: 13430 loss: 0.0070 lr: 0.02\n",
            "iteration: 13440 loss: 0.0044 lr: 0.02\n",
            "iteration: 13450 loss: 0.0037 lr: 0.02\n",
            "iteration: 13460 loss: 0.0037 lr: 0.02\n",
            "iteration: 13470 loss: 0.0051 lr: 0.02\n",
            "iteration: 13480 loss: 0.0040 lr: 0.02\n",
            "iteration: 13490 loss: 0.0040 lr: 0.02\n",
            "iteration: 13500 loss: 0.0051 lr: 0.02\n",
            "iteration: 13510 loss: 0.0047 lr: 0.02\n",
            "iteration: 13520 loss: 0.0038 lr: 0.02\n",
            "iteration: 13530 loss: 0.0045 lr: 0.02\n",
            "iteration: 13540 loss: 0.0041 lr: 0.02\n",
            "iteration: 13550 loss: 0.0033 lr: 0.02\n",
            "iteration: 13560 loss: 0.0056 lr: 0.02\n",
            "iteration: 13570 loss: 0.0050 lr: 0.02\n",
            "iteration: 13580 loss: 0.0059 lr: 0.02\n",
            "iteration: 13590 loss: 0.0046 lr: 0.02\n",
            "iteration: 13600 loss: 0.0048 lr: 0.02\n",
            "iteration: 13610 loss: 0.0041 lr: 0.02\n",
            "iteration: 13620 loss: 0.0039 lr: 0.02\n",
            "iteration: 13630 loss: 0.0055 lr: 0.02\n",
            "iteration: 13640 loss: 0.0038 lr: 0.02\n",
            "iteration: 13650 loss: 0.0039 lr: 0.02\n",
            "iteration: 13660 loss: 0.0048 lr: 0.02\n",
            "iteration: 13670 loss: 0.0043 lr: 0.02\n",
            "iteration: 13680 loss: 0.0046 lr: 0.02\n",
            "iteration: 13690 loss: 0.0046 lr: 0.02\n",
            "iteration: 13700 loss: 0.0036 lr: 0.02\n",
            "iteration: 13710 loss: 0.0053 lr: 0.02\n",
            "iteration: 13720 loss: 0.0038 lr: 0.02\n",
            "iteration: 13730 loss: 0.0039 lr: 0.02\n",
            "iteration: 13740 loss: 0.0042 lr: 0.02\n",
            "iteration: 13750 loss: 0.0055 lr: 0.02\n",
            "iteration: 13760 loss: 0.0033 lr: 0.02\n",
            "iteration: 13770 loss: 0.0044 lr: 0.02\n",
            "iteration: 13780 loss: 0.0036 lr: 0.02\n",
            "iteration: 13790 loss: 0.0034 lr: 0.02\n",
            "iteration: 13800 loss: 0.0041 lr: 0.02\n",
            "iteration: 13810 loss: 0.0042 lr: 0.02\n",
            "iteration: 13820 loss: 0.0037 lr: 0.02\n",
            "iteration: 13830 loss: 0.0038 lr: 0.02\n",
            "iteration: 13840 loss: 0.0041 lr: 0.02\n",
            "iteration: 13850 loss: 0.0037 lr: 0.02\n",
            "iteration: 13860 loss: 0.0034 lr: 0.02\n",
            "iteration: 13870 loss: 0.0040 lr: 0.02\n",
            "iteration: 13880 loss: 0.0051 lr: 0.02\n",
            "iteration: 13890 loss: 0.0035 lr: 0.02\n",
            "iteration: 13900 loss: 0.0033 lr: 0.02\n",
            "iteration: 13910 loss: 0.0038 lr: 0.02\n",
            "iteration: 13920 loss: 0.0025 lr: 0.02\n",
            "iteration: 13930 loss: 0.0041 lr: 0.02\n",
            "iteration: 13940 loss: 0.0043 lr: 0.02\n",
            "iteration: 13950 loss: 0.0041 lr: 0.02\n",
            "iteration: 13960 loss: 0.0034 lr: 0.02\n",
            "iteration: 13970 loss: 0.0043 lr: 0.02\n",
            "iteration: 13980 loss: 0.0045 lr: 0.02\n",
            "iteration: 13990 loss: 0.0043 lr: 0.02\n",
            "iteration: 14000 loss: 0.0045 lr: 0.02\n",
            "iteration: 14010 loss: 0.0043 lr: 0.02\n",
            "iteration: 14020 loss: 0.0039 lr: 0.02\n",
            "iteration: 14030 loss: 0.0042 lr: 0.02\n",
            "iteration: 14040 loss: 0.0049 lr: 0.02\n",
            "iteration: 14050 loss: 0.0041 lr: 0.02\n",
            "iteration: 14060 loss: 0.0039 lr: 0.02\n",
            "iteration: 14070 loss: 0.0033 lr: 0.02\n",
            "iteration: 14080 loss: 0.0033 lr: 0.02\n",
            "iteration: 14090 loss: 0.0031 lr: 0.02\n",
            "iteration: 14100 loss: 0.0034 lr: 0.02\n",
            "iteration: 14110 loss: 0.0038 lr: 0.02\n",
            "iteration: 14120 loss: 0.0037 lr: 0.02\n",
            "iteration: 14130 loss: 0.0035 lr: 0.02\n",
            "iteration: 14140 loss: 0.0041 lr: 0.02\n",
            "iteration: 14150 loss: 0.0034 lr: 0.02\n",
            "iteration: 14160 loss: 0.0038 lr: 0.02\n",
            "iteration: 14170 loss: 0.0039 lr: 0.02\n",
            "iteration: 14180 loss: 0.0028 lr: 0.02\n",
            "iteration: 14190 loss: 0.0031 lr: 0.02\n",
            "iteration: 14200 loss: 0.0029 lr: 0.02\n",
            "iteration: 14210 loss: 0.0042 lr: 0.02\n",
            "iteration: 14220 loss: 0.0043 lr: 0.02\n",
            "iteration: 14230 loss: 0.0028 lr: 0.02\n",
            "iteration: 14240 loss: 0.0033 lr: 0.02\n",
            "iteration: 14250 loss: 0.0030 lr: 0.02\n",
            "iteration: 14260 loss: 0.0029 lr: 0.02\n",
            "iteration: 14270 loss: 0.0036 lr: 0.02\n",
            "iteration: 14280 loss: 0.0052 lr: 0.02\n",
            "iteration: 14290 loss: 0.0033 lr: 0.02\n",
            "iteration: 14300 loss: 0.0033 lr: 0.02\n",
            "iteration: 14310 loss: 0.0030 lr: 0.02\n",
            "iteration: 14320 loss: 0.0036 lr: 0.02\n",
            "iteration: 14330 loss: 0.0031 lr: 0.02\n",
            "iteration: 14340 loss: 0.0035 lr: 0.02\n",
            "iteration: 14350 loss: 0.0052 lr: 0.02\n",
            "iteration: 14360 loss: 0.0057 lr: 0.02\n",
            "iteration: 14370 loss: 0.0046 lr: 0.02\n",
            "iteration: 14380 loss: 0.0042 lr: 0.02\n",
            "iteration: 14390 loss: 0.0036 lr: 0.02\n",
            "iteration: 14400 loss: 0.0051 lr: 0.02\n",
            "iteration: 14410 loss: 0.0039 lr: 0.02\n",
            "iteration: 14420 loss: 0.0065 lr: 0.02\n",
            "iteration: 14430 loss: 0.0052 lr: 0.02\n",
            "iteration: 14440 loss: 0.0048 lr: 0.02\n",
            "iteration: 14450 loss: 0.0042 lr: 0.02\n",
            "iteration: 14460 loss: 0.0036 lr: 0.02\n",
            "iteration: 14470 loss: 0.0049 lr: 0.02\n",
            "iteration: 14480 loss: 0.0044 lr: 0.02\n",
            "iteration: 14490 loss: 0.0039 lr: 0.02\n",
            "iteration: 14500 loss: 0.0039 lr: 0.02\n",
            "iteration: 14510 loss: 0.0037 lr: 0.02\n",
            "iteration: 14520 loss: 0.0050 lr: 0.02\n",
            "iteration: 14530 loss: 0.0049 lr: 0.02\n",
            "iteration: 14540 loss: 0.0040 lr: 0.02\n",
            "iteration: 14550 loss: 0.0046 lr: 0.02\n",
            "iteration: 14560 loss: 0.0023 lr: 0.02\n",
            "iteration: 14570 loss: 0.0044 lr: 0.02\n",
            "iteration: 14580 loss: 0.0047 lr: 0.02\n",
            "iteration: 14590 loss: 0.0033 lr: 0.02\n",
            "iteration: 14600 loss: 0.0035 lr: 0.02\n",
            "iteration: 14610 loss: 0.0028 lr: 0.02\n",
            "iteration: 14620 loss: 0.0033 lr: 0.02\n",
            "iteration: 14630 loss: 0.0034 lr: 0.02\n",
            "iteration: 14640 loss: 0.0037 lr: 0.02\n",
            "iteration: 14650 loss: 0.0068 lr: 0.02\n",
            "iteration: 14660 loss: 0.0072 lr: 0.02\n",
            "iteration: 14670 loss: 0.0050 lr: 0.02\n",
            "iteration: 14680 loss: 0.0050 lr: 0.02\n",
            "iteration: 14690 loss: 0.0042 lr: 0.02\n",
            "iteration: 14700 loss: 0.0037 lr: 0.02\n",
            "iteration: 14710 loss: 0.0037 lr: 0.02\n",
            "iteration: 14720 loss: 0.0037 lr: 0.02\n",
            "iteration: 14730 loss: 0.0031 lr: 0.02\n",
            "iteration: 14740 loss: 0.0027 lr: 0.02\n",
            "iteration: 14750 loss: 0.0035 lr: 0.02\n",
            "iteration: 14760 loss: 0.0029 lr: 0.02\n",
            "iteration: 14770 loss: 0.0024 lr: 0.02\n",
            "iteration: 14780 loss: 0.0034 lr: 0.02\n",
            "iteration: 14790 loss: 0.0038 lr: 0.02\n",
            "iteration: 14800 loss: 0.0029 lr: 0.02\n",
            "iteration: 14810 loss: 0.0025 lr: 0.02\n",
            "iteration: 14820 loss: 0.0035 lr: 0.02\n",
            "iteration: 14830 loss: 0.0029 lr: 0.02\n",
            "iteration: 14840 loss: 0.0034 lr: 0.02\n",
            "iteration: 14850 loss: 0.0046 lr: 0.02\n",
            "iteration: 14860 loss: 0.0038 lr: 0.02\n",
            "iteration: 14870 loss: 0.0068 lr: 0.02\n",
            "iteration: 14880 loss: 0.0048 lr: 0.02\n",
            "iteration: 14890 loss: 0.0038 lr: 0.02\n",
            "iteration: 14900 loss: 0.0042 lr: 0.02\n",
            "iteration: 14910 loss: 0.0042 lr: 0.02\n",
            "iteration: 14920 loss: 0.0036 lr: 0.02\n",
            "iteration: 14930 loss: 0.0055 lr: 0.02\n",
            "iteration: 14940 loss: 0.0035 lr: 0.02\n",
            "iteration: 14950 loss: 0.0048 lr: 0.02\n",
            "iteration: 14960 loss: 0.0037 lr: 0.02\n",
            "iteration: 14970 loss: 0.0085 lr: 0.02\n",
            "iteration: 14980 loss: 0.0042 lr: 0.02\n",
            "iteration: 14990 loss: 0.0035 lr: 0.02\n",
            "iteration: 15000 loss: 0.0034 lr: 0.02\n",
            "iteration: 15010 loss: 0.0041 lr: 0.02\n",
            "iteration: 15020 loss: 0.0033 lr: 0.02\n",
            "iteration: 15030 loss: 0.0030 lr: 0.02\n",
            "iteration: 15040 loss: 0.0035 lr: 0.02\n",
            "iteration: 15050 loss: 0.0040 lr: 0.02\n",
            "iteration: 15060 loss: 0.0029 lr: 0.02\n",
            "iteration: 15070 loss: 0.0040 lr: 0.02\n",
            "iteration: 15080 loss: 0.0041 lr: 0.02\n",
            "iteration: 15090 loss: 0.0052 lr: 0.02\n",
            "iteration: 15100 loss: 0.0032 lr: 0.02\n",
            "iteration: 15110 loss: 0.0028 lr: 0.02\n",
            "iteration: 15120 loss: 0.0033 lr: 0.02\n",
            "iteration: 15130 loss: 0.0042 lr: 0.02\n",
            "iteration: 15140 loss: 0.0062 lr: 0.02\n",
            "iteration: 15150 loss: 0.0033 lr: 0.02\n",
            "iteration: 15160 loss: 0.0040 lr: 0.02\n",
            "iteration: 15170 loss: 0.0031 lr: 0.02\n",
            "iteration: 15180 loss: 0.0044 lr: 0.02\n",
            "iteration: 15190 loss: 0.0040 lr: 0.02\n",
            "iteration: 15200 loss: 0.0033 lr: 0.02\n",
            "iteration: 15210 loss: 0.0039 lr: 0.02\n",
            "iteration: 15220 loss: 0.0035 lr: 0.02\n",
            "iteration: 15230 loss: 0.0035 lr: 0.02\n",
            "iteration: 15240 loss: 0.0055 lr: 0.02\n",
            "iteration: 15250 loss: 0.0037 lr: 0.02\n",
            "iteration: 15260 loss: 0.0030 lr: 0.02\n",
            "iteration: 15270 loss: 0.0033 lr: 0.02\n",
            "iteration: 15280 loss: 0.0033 lr: 0.02\n",
            "iteration: 15290 loss: 0.0027 lr: 0.02\n",
            "iteration: 15300 loss: 0.0040 lr: 0.02\n",
            "iteration: 15310 loss: 0.0032 lr: 0.02\n",
            "iteration: 15320 loss: 0.0029 lr: 0.02\n",
            "iteration: 15330 loss: 0.0048 lr: 0.02\n",
            "iteration: 15340 loss: 0.0029 lr: 0.02\n",
            "iteration: 15350 loss: 0.0044 lr: 0.02\n",
            "iteration: 15360 loss: 0.0044 lr: 0.02\n",
            "iteration: 15370 loss: 0.0038 lr: 0.02\n",
            "iteration: 15380 loss: 0.0042 lr: 0.02\n",
            "iteration: 15390 loss: 0.0032 lr: 0.02\n",
            "iteration: 15400 loss: 0.0029 lr: 0.02\n",
            "iteration: 15410 loss: 0.0032 lr: 0.02\n",
            "iteration: 15420 loss: 0.0032 lr: 0.02\n",
            "iteration: 15430 loss: 0.0045 lr: 0.02\n",
            "iteration: 15440 loss: 0.0040 lr: 0.02\n",
            "iteration: 15450 loss: 0.0033 lr: 0.02\n",
            "iteration: 15460 loss: 0.0033 lr: 0.02\n",
            "iteration: 15470 loss: 0.0038 lr: 0.02\n",
            "iteration: 15480 loss: 0.0038 lr: 0.02\n",
            "iteration: 15490 loss: 0.0041 lr: 0.02\n",
            "iteration: 15500 loss: 0.0037 lr: 0.02\n",
            "iteration: 15510 loss: 0.0021 lr: 0.02\n",
            "iteration: 15520 loss: 0.0030 lr: 0.02\n",
            "iteration: 15530 loss: 0.0055 lr: 0.02\n",
            "iteration: 15540 loss: 0.0035 lr: 0.02\n",
            "iteration: 15550 loss: 0.0033 lr: 0.02\n",
            "iteration: 15560 loss: 0.0030 lr: 0.02\n",
            "iteration: 15570 loss: 0.0032 lr: 0.02\n",
            "iteration: 15580 loss: 0.0029 lr: 0.02\n",
            "iteration: 15590 loss: 0.0029 lr: 0.02\n",
            "iteration: 15600 loss: 0.0040 lr: 0.02\n",
            "iteration: 15610 loss: 0.0036 lr: 0.02\n",
            "iteration: 15620 loss: 0.0034 lr: 0.02\n",
            "iteration: 15630 loss: 0.0030 lr: 0.02\n",
            "iteration: 15640 loss: 0.0029 lr: 0.02\n",
            "iteration: 15650 loss: 0.0028 lr: 0.02\n",
            "iteration: 15660 loss: 0.0032 lr: 0.02\n",
            "iteration: 15670 loss: 0.0028 lr: 0.02\n",
            "iteration: 15680 loss: 0.0025 lr: 0.02\n",
            "iteration: 15690 loss: 0.0036 lr: 0.02\n",
            "iteration: 15700 loss: 0.0036 lr: 0.02\n",
            "iteration: 15710 loss: 0.0036 lr: 0.02\n",
            "iteration: 15720 loss: 0.0038 lr: 0.02\n",
            "iteration: 15730 loss: 0.0033 lr: 0.02\n",
            "iteration: 15740 loss: 0.0035 lr: 0.02\n",
            "iteration: 15750 loss: 0.0030 lr: 0.02\n",
            "iteration: 15760 loss: 0.0027 lr: 0.02\n",
            "iteration: 15770 loss: 0.0035 lr: 0.02\n",
            "iteration: 15780 loss: 0.0042 lr: 0.02\n",
            "iteration: 15790 loss: 0.0028 lr: 0.02\n",
            "iteration: 15800 loss: 0.0028 lr: 0.02\n",
            "iteration: 15810 loss: 0.0039 lr: 0.02\n",
            "iteration: 15820 loss: 0.0040 lr: 0.02\n",
            "iteration: 15830 loss: 0.0032 lr: 0.02\n",
            "iteration: 15840 loss: 0.0038 lr: 0.02\n",
            "iteration: 15850 loss: 0.0036 lr: 0.02\n",
            "iteration: 15860 loss: 0.0025 lr: 0.02\n",
            "iteration: 15870 loss: 0.0040 lr: 0.02\n",
            "iteration: 15880 loss: 0.0040 lr: 0.02\n",
            "iteration: 15890 loss: 0.0044 lr: 0.02\n",
            "iteration: 15900 loss: 0.0030 lr: 0.02\n",
            "iteration: 15910 loss: 0.0032 lr: 0.02\n",
            "iteration: 15920 loss: 0.0031 lr: 0.02\n",
            "iteration: 15930 loss: 0.0039 lr: 0.02\n",
            "iteration: 15940 loss: 0.0036 lr: 0.02\n",
            "iteration: 15950 loss: 0.0033 lr: 0.02\n",
            "iteration: 15960 loss: 0.0029 lr: 0.02\n",
            "iteration: 15970 loss: 0.0040 lr: 0.02\n",
            "iteration: 15980 loss: 0.0077 lr: 0.02\n",
            "iteration: 15990 loss: 0.0047 lr: 0.02\n",
            "iteration: 16000 loss: 0.0028 lr: 0.02\n",
            "iteration: 16010 loss: 0.0035 lr: 0.02\n",
            "iteration: 16020 loss: 0.0044 lr: 0.02\n",
            "iteration: 16030 loss: 0.0041 lr: 0.02\n",
            "iteration: 16040 loss: 0.0044 lr: 0.02\n",
            "iteration: 16050 loss: 0.0035 lr: 0.02\n",
            "iteration: 16060 loss: 0.0035 lr: 0.02\n",
            "iteration: 16070 loss: 0.0038 lr: 0.02\n",
            "iteration: 16080 loss: 0.0035 lr: 0.02\n",
            "iteration: 16090 loss: 0.0032 lr: 0.02\n",
            "iteration: 16100 loss: 0.0032 lr: 0.02\n",
            "iteration: 16110 loss: 0.0030 lr: 0.02\n",
            "iteration: 16120 loss: 0.0032 lr: 0.02\n",
            "iteration: 16130 loss: 0.0035 lr: 0.02\n",
            "iteration: 16140 loss: 0.0036 lr: 0.02\n",
            "iteration: 16150 loss: 0.0040 lr: 0.02\n",
            "iteration: 16160 loss: 0.0028 lr: 0.02\n",
            "iteration: 16170 loss: 0.0038 lr: 0.02\n",
            "iteration: 16180 loss: 0.0030 lr: 0.02\n",
            "iteration: 16190 loss: 0.0034 lr: 0.02\n",
            "iteration: 16200 loss: 0.0039 lr: 0.02\n",
            "iteration: 16210 loss: 0.0045 lr: 0.02\n",
            "iteration: 16220 loss: 0.0035 lr: 0.02\n",
            "iteration: 16230 loss: 0.0030 lr: 0.02\n",
            "iteration: 16240 loss: 0.0027 lr: 0.02\n",
            "iteration: 16250 loss: 0.0027 lr: 0.02\n",
            "iteration: 16260 loss: 0.0035 lr: 0.02\n",
            "iteration: 16270 loss: 0.0039 lr: 0.02\n",
            "iteration: 16280 loss: 0.0032 lr: 0.02\n",
            "iteration: 16290 loss: 0.0035 lr: 0.02\n",
            "iteration: 16300 loss: 0.0040 lr: 0.02\n",
            "iteration: 16310 loss: 0.0028 lr: 0.02\n",
            "iteration: 16320 loss: 0.0027 lr: 0.02\n",
            "iteration: 16330 loss: 0.0027 lr: 0.02\n",
            "iteration: 16340 loss: 0.0033 lr: 0.02\n",
            "iteration: 16350 loss: 0.0040 lr: 0.02\n",
            "iteration: 16360 loss: 0.0034 lr: 0.02\n",
            "iteration: 16370 loss: 0.0036 lr: 0.02\n",
            "iteration: 16380 loss: 0.0033 lr: 0.02\n",
            "iteration: 16390 loss: 0.0033 lr: 0.02\n",
            "iteration: 16400 loss: 0.0030 lr: 0.02\n",
            "iteration: 16410 loss: 0.0032 lr: 0.02\n",
            "iteration: 16420 loss: 0.0027 lr: 0.02\n",
            "iteration: 16430 loss: 0.0033 lr: 0.02\n",
            "iteration: 16440 loss: 0.0035 lr: 0.02\n",
            "iteration: 16450 loss: 0.0038 lr: 0.02\n",
            "iteration: 16460 loss: 0.0035 lr: 0.02\n",
            "iteration: 16470 loss: 0.0032 lr: 0.02\n",
            "iteration: 16480 loss: 0.0033 lr: 0.02\n",
            "iteration: 16490 loss: 0.0031 lr: 0.02\n",
            "iteration: 16500 loss: 0.0037 lr: 0.02\n",
            "iteration: 16510 loss: 0.0038 lr: 0.02\n",
            "iteration: 16520 loss: 0.0041 lr: 0.02\n",
            "iteration: 16530 loss: 0.0039 lr: 0.02\n",
            "iteration: 16540 loss: 0.0029 lr: 0.02\n",
            "iteration: 16550 loss: 0.0035 lr: 0.02\n",
            "iteration: 16560 loss: 0.0031 lr: 0.02\n",
            "iteration: 16570 loss: 0.0027 lr: 0.02\n",
            "iteration: 16580 loss: 0.0027 lr: 0.02\n",
            "iteration: 16590 loss: 0.0035 lr: 0.02\n",
            "iteration: 16600 loss: 0.0034 lr: 0.02\n",
            "iteration: 16610 loss: 0.0038 lr: 0.02\n",
            "iteration: 16620 loss: 0.0039 lr: 0.02\n",
            "iteration: 16630 loss: 0.0051 lr: 0.02\n",
            "iteration: 16640 loss: 0.0038 lr: 0.02\n",
            "iteration: 16650 loss: 0.0038 lr: 0.02\n",
            "iteration: 16660 loss: 0.0029 lr: 0.02\n",
            "iteration: 16670 loss: 0.0033 lr: 0.02\n",
            "iteration: 16680 loss: 0.0032 lr: 0.02\n",
            "iteration: 16690 loss: 0.0035 lr: 0.02\n",
            "iteration: 16700 loss: 0.0028 lr: 0.02\n",
            "iteration: 16710 loss: 0.0043 lr: 0.02\n",
            "iteration: 16720 loss: 0.0038 lr: 0.02\n",
            "iteration: 16730 loss: 0.0035 lr: 0.02\n",
            "iteration: 16740 loss: 0.0033 lr: 0.02\n",
            "iteration: 16750 loss: 0.0043 lr: 0.02\n",
            "iteration: 16760 loss: 0.0030 lr: 0.02\n",
            "iteration: 16770 loss: 0.0032 lr: 0.02\n",
            "iteration: 16780 loss: 0.0028 lr: 0.02\n",
            "iteration: 16790 loss: 0.0030 lr: 0.02\n",
            "iteration: 16800 loss: 0.0030 lr: 0.02\n",
            "iteration: 16810 loss: 0.0044 lr: 0.02\n",
            "iteration: 16820 loss: 0.0031 lr: 0.02\n",
            "iteration: 16830 loss: 0.0031 lr: 0.02\n",
            "iteration: 16840 loss: 0.0027 lr: 0.02\n",
            "iteration: 16850 loss: 0.0047 lr: 0.02\n",
            "iteration: 16860 loss: 0.0039 lr: 0.02\n",
            "iteration: 16870 loss: 0.0035 lr: 0.02\n",
            "iteration: 16880 loss: 0.0033 lr: 0.02\n",
            "iteration: 16890 loss: 0.0035 lr: 0.02\n",
            "iteration: 16900 loss: 0.0050 lr: 0.02\n",
            "iteration: 16910 loss: 0.0037 lr: 0.02\n",
            "iteration: 16920 loss: 0.0047 lr: 0.02\n",
            "iteration: 16930 loss: 0.0033 lr: 0.02\n",
            "iteration: 16940 loss: 0.0039 lr: 0.02\n",
            "iteration: 16950 loss: 0.0025 lr: 0.02\n",
            "iteration: 16960 loss: 0.0037 lr: 0.02\n",
            "iteration: 16970 loss: 0.0049 lr: 0.02\n",
            "iteration: 16980 loss: 0.0034 lr: 0.02\n",
            "iteration: 16990 loss: 0.0032 lr: 0.02\n",
            "iteration: 17000 loss: 0.0041 lr: 0.02\n",
            "iteration: 17010 loss: 0.0035 lr: 0.02\n",
            "iteration: 17020 loss: 0.0037 lr: 0.02\n",
            "iteration: 17030 loss: 0.0027 lr: 0.02\n",
            "iteration: 17040 loss: 0.0029 lr: 0.02\n",
            "iteration: 17050 loss: 0.0038 lr: 0.02\n",
            "iteration: 17060 loss: 0.0036 lr: 0.02\n",
            "iteration: 17070 loss: 0.0033 lr: 0.02\n",
            "iteration: 17080 loss: 0.0052 lr: 0.02\n",
            "iteration: 17090 loss: 0.0050 lr: 0.02\n",
            "iteration: 17100 loss: 0.0032 lr: 0.02\n",
            "iteration: 17110 loss: 0.0040 lr: 0.02\n",
            "iteration: 17120 loss: 0.0038 lr: 0.02\n",
            "iteration: 17130 loss: 0.0027 lr: 0.02\n",
            "iteration: 17140 loss: 0.0037 lr: 0.02\n",
            "iteration: 17150 loss: 0.0031 lr: 0.02\n",
            "iteration: 17160 loss: 0.0051 lr: 0.02\n",
            "iteration: 17170 loss: 0.0030 lr: 0.02\n",
            "iteration: 17180 loss: 0.0037 lr: 0.02\n",
            "iteration: 17190 loss: 0.0035 lr: 0.02\n",
            "iteration: 17200 loss: 0.0042 lr: 0.02\n",
            "iteration: 17210 loss: 0.0025 lr: 0.02\n",
            "iteration: 17220 loss: 0.0028 lr: 0.02\n",
            "iteration: 17230 loss: 0.0040 lr: 0.02\n",
            "iteration: 17240 loss: 0.0044 lr: 0.02\n",
            "iteration: 17250 loss: 0.0028 lr: 0.02\n",
            "iteration: 17260 loss: 0.0052 lr: 0.02\n",
            "iteration: 17270 loss: 0.0028 lr: 0.02\n",
            "iteration: 17280 loss: 0.0032 lr: 0.02\n",
            "iteration: 17290 loss: 0.0035 lr: 0.02\n",
            "iteration: 17300 loss: 0.0034 lr: 0.02\n",
            "iteration: 17310 loss: 0.0024 lr: 0.02\n",
            "iteration: 17320 loss: 0.0039 lr: 0.02\n",
            "iteration: 17330 loss: 0.0037 lr: 0.02\n",
            "iteration: 17340 loss: 0.0033 lr: 0.02\n",
            "iteration: 17350 loss: 0.0037 lr: 0.02\n",
            "iteration: 17360 loss: 0.0034 lr: 0.02\n",
            "iteration: 17370 loss: 0.0032 lr: 0.02\n",
            "iteration: 17380 loss: 0.0038 lr: 0.02\n",
            "iteration: 17390 loss: 0.0035 lr: 0.02\n",
            "iteration: 17400 loss: 0.0025 lr: 0.02\n",
            "iteration: 17410 loss: 0.0030 lr: 0.02\n",
            "iteration: 17420 loss: 0.0031 lr: 0.02\n",
            "iteration: 17430 loss: 0.0031 lr: 0.02\n",
            "iteration: 17440 loss: 0.0036 lr: 0.02\n",
            "iteration: 17450 loss: 0.0032 lr: 0.02\n",
            "iteration: 17460 loss: 0.0032 lr: 0.02\n",
            "iteration: 17470 loss: 0.0030 lr: 0.02\n",
            "iteration: 17480 loss: 0.0027 lr: 0.02\n",
            "iteration: 17490 loss: 0.0034 lr: 0.02\n",
            "iteration: 17500 loss: 0.0036 lr: 0.02\n",
            "iteration: 17510 loss: 0.0032 lr: 0.02\n",
            "iteration: 17520 loss: 0.0044 lr: 0.02\n",
            "iteration: 17530 loss: 0.0032 lr: 0.02\n",
            "iteration: 17540 loss: 0.0027 lr: 0.02\n",
            "iteration: 17550 loss: 0.0038 lr: 0.02\n",
            "iteration: 17560 loss: 0.0030 lr: 0.02\n",
            "iteration: 17570 loss: 0.0035 lr: 0.02\n",
            "iteration: 17580 loss: 0.0032 lr: 0.02\n",
            "iteration: 17590 loss: 0.0029 lr: 0.02\n",
            "iteration: 17600 loss: 0.0023 lr: 0.02\n",
            "iteration: 17610 loss: 0.0027 lr: 0.02\n",
            "iteration: 17620 loss: 0.0027 lr: 0.02\n",
            "iteration: 17630 loss: 0.0030 lr: 0.02\n",
            "iteration: 17640 loss: 0.0033 lr: 0.02\n",
            "iteration: 17650 loss: 0.0031 lr: 0.02\n",
            "iteration: 17660 loss: 0.0037 lr: 0.02\n",
            "iteration: 17670 loss: 0.0025 lr: 0.02\n",
            "iteration: 17680 loss: 0.0032 lr: 0.02\n",
            "iteration: 17690 loss: 0.0039 lr: 0.02\n",
            "iteration: 17700 loss: 0.0027 lr: 0.02\n",
            "iteration: 17710 loss: 0.0041 lr: 0.02\n",
            "iteration: 17720 loss: 0.0026 lr: 0.02\n",
            "iteration: 17730 loss: 0.0022 lr: 0.02\n",
            "iteration: 17740 loss: 0.0035 lr: 0.02\n",
            "iteration: 17750 loss: 0.0033 lr: 0.02\n",
            "iteration: 17760 loss: 0.0029 lr: 0.02\n",
            "iteration: 17770 loss: 0.0032 lr: 0.02\n",
            "iteration: 17780 loss: 0.0034 lr: 0.02\n",
            "iteration: 17790 loss: 0.0036 lr: 0.02\n",
            "iteration: 17800 loss: 0.0047 lr: 0.02\n",
            "iteration: 17810 loss: 0.0030 lr: 0.02\n",
            "iteration: 17820 loss: 0.0032 lr: 0.02\n",
            "iteration: 17830 loss: 0.0043 lr: 0.02\n",
            "iteration: 17840 loss: 0.0029 lr: 0.02\n",
            "iteration: 17850 loss: 0.0030 lr: 0.02\n",
            "iteration: 17860 loss: 0.0023 lr: 0.02\n",
            "iteration: 17870 loss: 0.0041 lr: 0.02\n",
            "iteration: 17880 loss: 0.0042 lr: 0.02\n",
            "iteration: 17890 loss: 0.0035 lr: 0.02\n",
            "iteration: 17900 loss: 0.0044 lr: 0.02\n",
            "iteration: 17910 loss: 0.0031 lr: 0.02\n",
            "iteration: 17920 loss: 0.0036 lr: 0.02\n",
            "iteration: 17930 loss: 0.0038 lr: 0.02\n",
            "iteration: 17940 loss: 0.0034 lr: 0.02\n",
            "iteration: 17950 loss: 0.0034 lr: 0.02\n",
            "iteration: 17960 loss: 0.0073 lr: 0.02\n",
            "iteration: 17970 loss: 0.0032 lr: 0.02\n",
            "iteration: 17980 loss: 0.0034 lr: 0.02\n",
            "iteration: 17990 loss: 0.0036 lr: 0.02\n",
            "iteration: 18000 loss: 0.0047 lr: 0.02\n",
            "iteration: 18010 loss: 0.0031 lr: 0.02\n",
            "iteration: 18020 loss: 0.0028 lr: 0.02\n",
            "iteration: 18030 loss: 0.0037 lr: 0.02\n",
            "iteration: 18040 loss: 0.0034 lr: 0.02\n",
            "iteration: 18050 loss: 0.0024 lr: 0.02\n",
            "iteration: 18060 loss: 0.0034 lr: 0.02\n",
            "iteration: 18070 loss: 0.0030 lr: 0.02\n",
            "iteration: 18080 loss: 0.0028 lr: 0.02\n",
            "iteration: 18090 loss: 0.0036 lr: 0.02\n",
            "iteration: 18100 loss: 0.0031 lr: 0.02\n",
            "iteration: 18110 loss: 0.0026 lr: 0.02\n",
            "iteration: 18120 loss: 0.0030 lr: 0.02\n",
            "iteration: 18130 loss: 0.0036 lr: 0.02\n",
            "iteration: 18140 loss: 0.0030 lr: 0.02\n",
            "iteration: 18150 loss: 0.0033 lr: 0.02\n",
            "iteration: 18160 loss: 0.0030 lr: 0.02\n",
            "iteration: 18170 loss: 0.0037 lr: 0.02\n",
            "iteration: 18180 loss: 0.0031 lr: 0.02\n",
            "iteration: 18190 loss: 0.0030 lr: 0.02\n",
            "iteration: 18200 loss: 0.0026 lr: 0.02\n",
            "iteration: 18210 loss: 0.0032 lr: 0.02\n",
            "iteration: 18220 loss: 0.0037 lr: 0.02\n",
            "iteration: 18230 loss: 0.0038 lr: 0.02\n",
            "iteration: 18240 loss: 0.0032 lr: 0.02\n",
            "iteration: 18250 loss: 0.0034 lr: 0.02\n",
            "iteration: 18260 loss: 0.0030 lr: 0.02\n",
            "iteration: 18270 loss: 0.0028 lr: 0.02\n",
            "iteration: 18280 loss: 0.0033 lr: 0.02\n",
            "iteration: 18290 loss: 0.0056 lr: 0.02\n",
            "iteration: 18300 loss: 0.0037 lr: 0.02\n",
            "iteration: 18310 loss: 0.0026 lr: 0.02\n",
            "iteration: 18320 loss: 0.0050 lr: 0.02\n",
            "iteration: 18330 loss: 0.0024 lr: 0.02\n",
            "iteration: 18340 loss: 0.0030 lr: 0.02\n",
            "iteration: 18350 loss: 0.0032 lr: 0.02\n",
            "iteration: 18360 loss: 0.0030 lr: 0.02\n",
            "iteration: 18370 loss: 0.0041 lr: 0.02\n",
            "iteration: 18380 loss: 0.0034 lr: 0.02\n",
            "iteration: 18390 loss: 0.0035 lr: 0.02\n",
            "iteration: 18400 loss: 0.0034 lr: 0.02\n",
            "iteration: 18410 loss: 0.0020 lr: 0.02\n",
            "iteration: 18420 loss: 0.0042 lr: 0.02\n",
            "iteration: 18430 loss: 0.0035 lr: 0.02\n",
            "iteration: 18440 loss: 0.0034 lr: 0.02\n",
            "iteration: 18450 loss: 0.0025 lr: 0.02\n",
            "iteration: 18460 loss: 0.0033 lr: 0.02\n",
            "iteration: 18470 loss: 0.0046 lr: 0.02\n",
            "iteration: 18480 loss: 0.0039 lr: 0.02\n",
            "iteration: 18490 loss: 0.0032 lr: 0.02\n",
            "iteration: 18500 loss: 0.0029 lr: 0.02\n",
            "iteration: 18510 loss: 0.0036 lr: 0.02\n",
            "iteration: 18520 loss: 0.0046 lr: 0.02\n",
            "iteration: 18530 loss: 0.0030 lr: 0.02\n",
            "iteration: 18540 loss: 0.0031 lr: 0.02\n",
            "iteration: 18550 loss: 0.0035 lr: 0.02\n",
            "iteration: 18560 loss: 0.0034 lr: 0.02\n",
            "iteration: 18570 loss: 0.0044 lr: 0.02\n",
            "iteration: 18580 loss: 0.0032 lr: 0.02\n",
            "iteration: 18590 loss: 0.0027 lr: 0.02\n",
            "iteration: 18600 loss: 0.0028 lr: 0.02\n",
            "iteration: 18610 loss: 0.0033 lr: 0.02\n",
            "iteration: 18620 loss: 0.0035 lr: 0.02\n",
            "iteration: 18630 loss: 0.0037 lr: 0.02\n",
            "iteration: 18640 loss: 0.0031 lr: 0.02\n",
            "iteration: 18650 loss: 0.0031 lr: 0.02\n",
            "iteration: 18660 loss: 0.0042 lr: 0.02\n",
            "iteration: 18670 loss: 0.0039 lr: 0.02\n",
            "iteration: 18680 loss: 0.0032 lr: 0.02\n",
            "iteration: 18690 loss: 0.0049 lr: 0.02\n",
            "iteration: 18700 loss: 0.0031 lr: 0.02\n",
            "iteration: 18710 loss: 0.0026 lr: 0.02\n",
            "iteration: 18720 loss: 0.0031 lr: 0.02\n",
            "iteration: 18730 loss: 0.0039 lr: 0.02\n",
            "iteration: 18740 loss: 0.0024 lr: 0.02\n",
            "iteration: 18750 loss: 0.0038 lr: 0.02\n",
            "iteration: 18760 loss: 0.0035 lr: 0.02\n",
            "iteration: 18770 loss: 0.0033 lr: 0.02\n",
            "iteration: 18780 loss: 0.0038 lr: 0.02\n",
            "iteration: 18790 loss: 0.0032 lr: 0.02\n",
            "iteration: 18800 loss: 0.0033 lr: 0.02\n",
            "iteration: 18810 loss: 0.0031 lr: 0.02\n",
            "iteration: 18820 loss: 0.0036 lr: 0.02\n",
            "iteration: 18830 loss: 0.0028 lr: 0.02\n",
            "iteration: 18840 loss: 0.0034 lr: 0.02\n",
            "iteration: 18850 loss: 0.0030 lr: 0.02\n",
            "iteration: 18860 loss: 0.0029 lr: 0.02\n",
            "iteration: 18870 loss: 0.0057 lr: 0.02\n",
            "iteration: 18880 loss: 0.0031 lr: 0.02\n",
            "iteration: 18890 loss: 0.0036 lr: 0.02\n",
            "iteration: 18900 loss: 0.0033 lr: 0.02\n",
            "iteration: 18910 loss: 0.0028 lr: 0.02\n",
            "iteration: 18920 loss: 0.0032 lr: 0.02\n",
            "iteration: 18930 loss: 0.0032 lr: 0.02\n",
            "iteration: 18940 loss: 0.0026 lr: 0.02\n",
            "iteration: 18950 loss: 0.0033 lr: 0.02\n",
            "iteration: 18960 loss: 0.0045 lr: 0.02\n",
            "iteration: 18970 loss: 0.0028 lr: 0.02\n",
            "iteration: 18980 loss: 0.0030 lr: 0.02\n",
            "iteration: 18990 loss: 0.0034 lr: 0.02\n",
            "iteration: 19000 loss: 0.0024 lr: 0.02\n",
            "iteration: 19010 loss: 0.0030 lr: 0.02\n",
            "iteration: 19020 loss: 0.0024 lr: 0.02\n",
            "iteration: 19030 loss: 0.0027 lr: 0.02\n",
            "iteration: 19040 loss: 0.0044 lr: 0.02\n",
            "iteration: 19050 loss: 0.0026 lr: 0.02\n",
            "iteration: 19060 loss: 0.0028 lr: 0.02\n",
            "iteration: 19070 loss: 0.0026 lr: 0.02\n",
            "iteration: 19080 loss: 0.0038 lr: 0.02\n",
            "iteration: 19090 loss: 0.0027 lr: 0.02\n",
            "iteration: 19100 loss: 0.0027 lr: 0.02\n",
            "iteration: 19110 loss: 0.0045 lr: 0.02\n",
            "iteration: 19120 loss: 0.0023 lr: 0.02\n",
            "iteration: 19130 loss: 0.0030 lr: 0.02\n",
            "iteration: 19140 loss: 0.0039 lr: 0.02\n",
            "iteration: 19150 loss: 0.0050 lr: 0.02\n",
            "iteration: 19160 loss: 0.0029 lr: 0.02\n",
            "iteration: 19170 loss: 0.0027 lr: 0.02\n",
            "iteration: 19180 loss: 0.0026 lr: 0.02\n",
            "iteration: 19190 loss: 0.0034 lr: 0.02\n",
            "iteration: 19200 loss: 0.0036 lr: 0.02\n",
            "iteration: 19210 loss: 0.0055 lr: 0.02\n",
            "iteration: 19220 loss: 0.0035 lr: 0.02\n",
            "iteration: 19230 loss: 0.0039 lr: 0.02\n",
            "iteration: 19240 loss: 0.0031 lr: 0.02\n",
            "iteration: 19250 loss: 0.0029 lr: 0.02\n",
            "iteration: 19260 loss: 0.0030 lr: 0.02\n",
            "iteration: 19270 loss: 0.0037 lr: 0.02\n",
            "iteration: 19280 loss: 0.0035 lr: 0.02\n",
            "iteration: 19290 loss: 0.0034 lr: 0.02\n",
            "iteration: 19300 loss: 0.0055 lr: 0.02\n",
            "iteration: 19310 loss: 0.0029 lr: 0.02\n",
            "iteration: 19320 loss: 0.0036 lr: 0.02\n",
            "iteration: 19330 loss: 0.0042 lr: 0.02\n",
            "iteration: 19340 loss: 0.0040 lr: 0.02\n",
            "iteration: 19350 loss: 0.0022 lr: 0.02\n",
            "iteration: 19360 loss: 0.0026 lr: 0.02\n",
            "iteration: 19370 loss: 0.0029 lr: 0.02\n",
            "iteration: 19380 loss: 0.0035 lr: 0.02\n",
            "iteration: 19390 loss: 0.0021 lr: 0.02\n",
            "iteration: 19400 loss: 0.0032 lr: 0.02\n",
            "iteration: 19410 loss: 0.0033 lr: 0.02\n",
            "iteration: 19420 loss: 0.0050 lr: 0.02\n",
            "iteration: 19430 loss: 0.0028 lr: 0.02\n",
            "iteration: 19440 loss: 0.0042 lr: 0.02\n",
            "iteration: 19450 loss: 0.0039 lr: 0.02\n",
            "iteration: 19460 loss: 0.0029 lr: 0.02\n",
            "iteration: 19470 loss: 0.0039 lr: 0.02\n",
            "iteration: 19480 loss: 0.0034 lr: 0.02\n",
            "iteration: 19490 loss: 0.0034 lr: 0.02\n",
            "iteration: 19500 loss: 0.0034 lr: 0.02\n",
            "iteration: 19510 loss: 0.0020 lr: 0.02\n",
            "iteration: 19520 loss: 0.0037 lr: 0.02\n",
            "iteration: 19530 loss: 0.0029 lr: 0.02\n",
            "iteration: 19540 loss: 0.0029 lr: 0.02\n",
            "iteration: 19550 loss: 0.0029 lr: 0.02\n",
            "iteration: 19560 loss: 0.0031 lr: 0.02\n",
            "iteration: 19570 loss: 0.0033 lr: 0.02\n",
            "iteration: 19580 loss: 0.0031 lr: 0.02\n",
            "iteration: 19590 loss: 0.0033 lr: 0.02\n",
            "iteration: 19600 loss: 0.0040 lr: 0.02\n",
            "iteration: 19610 loss: 0.0035 lr: 0.02\n",
            "iteration: 19620 loss: 0.0030 lr: 0.02\n",
            "iteration: 19630 loss: 0.0045 lr: 0.02\n",
            "iteration: 19640 loss: 0.0031 lr: 0.02\n",
            "iteration: 19650 loss: 0.0032 lr: 0.02\n",
            "iteration: 19660 loss: 0.0028 lr: 0.02\n",
            "iteration: 19670 loss: 0.0040 lr: 0.02\n",
            "iteration: 19680 loss: 0.0030 lr: 0.02\n",
            "iteration: 19690 loss: 0.0022 lr: 0.02\n",
            "iteration: 19700 loss: 0.0042 lr: 0.02\n",
            "iteration: 19710 loss: 0.0026 lr: 0.02\n",
            "iteration: 19720 loss: 0.0034 lr: 0.02\n",
            "iteration: 19730 loss: 0.0028 lr: 0.02\n",
            "iteration: 19740 loss: 0.0032 lr: 0.02\n",
            "iteration: 19750 loss: 0.0035 lr: 0.02\n",
            "iteration: 19760 loss: 0.0023 lr: 0.02\n",
            "iteration: 19770 loss: 0.0032 lr: 0.02\n",
            "iteration: 19780 loss: 0.0031 lr: 0.02\n",
            "iteration: 19790 loss: 0.0034 lr: 0.02\n",
            "iteration: 19800 loss: 0.0046 lr: 0.02\n",
            "iteration: 19810 loss: 0.0034 lr: 0.02\n",
            "iteration: 19820 loss: 0.0031 lr: 0.02\n",
            "iteration: 19830 loss: 0.0023 lr: 0.02\n",
            "iteration: 19840 loss: 0.0035 lr: 0.02\n",
            "iteration: 19850 loss: 0.0032 lr: 0.02\n",
            "iteration: 19860 loss: 0.0029 lr: 0.02\n",
            "iteration: 19870 loss: 0.0034 lr: 0.02\n",
            "iteration: 19880 loss: 0.0039 lr: 0.02\n",
            "iteration: 19890 loss: 0.0027 lr: 0.02\n",
            "iteration: 19900 loss: 0.0031 lr: 0.02\n",
            "iteration: 19910 loss: 0.0023 lr: 0.02\n",
            "iteration: 19920 loss: 0.0033 lr: 0.02\n",
            "iteration: 19930 loss: 0.0026 lr: 0.02\n",
            "iteration: 19940 loss: 0.0025 lr: 0.02\n",
            "iteration: 19950 loss: 0.0028 lr: 0.02\n",
            "iteration: 19960 loss: 0.0029 lr: 0.02\n",
            "iteration: 19970 loss: 0.0022 lr: 0.02\n",
            "iteration: 19980 loss: 0.0025 lr: 0.02\n",
            "iteration: 19990 loss: 0.0023 lr: 0.02\n",
            "iteration: 20000 loss: 0.0027 lr: 0.02\n",
            "iteration: 20010 loss: 0.0032 lr: 0.02\n",
            "iteration: 20020 loss: 0.0037 lr: 0.02\n",
            "iteration: 20030 loss: 0.0033 lr: 0.02\n",
            "iteration: 20040 loss: 0.0029 lr: 0.02\n",
            "iteration: 20050 loss: 0.0037 lr: 0.02\n",
            "iteration: 20060 loss: 0.0026 lr: 0.02\n",
            "iteration: 20070 loss: 0.0028 lr: 0.02\n",
            "iteration: 20080 loss: 0.0028 lr: 0.02\n",
            "iteration: 20090 loss: 0.0037 lr: 0.02\n",
            "iteration: 20100 loss: 0.0027 lr: 0.02\n",
            "iteration: 20110 loss: 0.0036 lr: 0.02\n",
            "iteration: 20120 loss: 0.0026 lr: 0.02\n",
            "iteration: 20130 loss: 0.0035 lr: 0.02\n",
            "iteration: 20140 loss: 0.0039 lr: 0.02\n",
            "iteration: 20150 loss: 0.0028 lr: 0.02\n",
            "iteration: 20160 loss: 0.0020 lr: 0.02\n",
            "iteration: 20170 loss: 0.0023 lr: 0.02\n",
            "iteration: 20180 loss: 0.0020 lr: 0.02\n",
            "iteration: 20190 loss: 0.0043 lr: 0.02\n",
            "iteration: 20200 loss: 0.0042 lr: 0.02\n",
            "iteration: 20210 loss: 0.0026 lr: 0.02\n",
            "iteration: 20220 loss: 0.0032 lr: 0.02\n",
            "iteration: 20230 loss: 0.0050 lr: 0.02\n",
            "iteration: 20240 loss: 0.0024 lr: 0.02\n",
            "iteration: 20250 loss: 0.0028 lr: 0.02\n",
            "iteration: 20260 loss: 0.0025 lr: 0.02\n",
            "iteration: 20270 loss: 0.0050 lr: 0.02\n",
            "iteration: 20280 loss: 0.0026 lr: 0.02\n",
            "iteration: 20290 loss: 0.0029 lr: 0.02\n",
            "iteration: 20300 loss: 0.0029 lr: 0.02\n",
            "iteration: 20310 loss: 0.0032 lr: 0.02\n",
            "iteration: 20320 loss: 0.0028 lr: 0.02\n",
            "iteration: 20330 loss: 0.0027 lr: 0.02\n",
            "iteration: 20340 loss: 0.0027 lr: 0.02\n",
            "iteration: 20350 loss: 0.0020 lr: 0.02\n",
            "iteration: 20360 loss: 0.0028 lr: 0.02\n",
            "iteration: 20370 loss: 0.0031 lr: 0.02\n",
            "iteration: 20380 loss: 0.0032 lr: 0.02\n",
            "iteration: 20390 loss: 0.0030 lr: 0.02\n",
            "iteration: 20400 loss: 0.0028 lr: 0.02\n",
            "iteration: 20410 loss: 0.0044 lr: 0.02\n",
            "iteration: 20420 loss: 0.0026 lr: 0.02\n",
            "iteration: 20430 loss: 0.0032 lr: 0.02\n",
            "iteration: 20440 loss: 0.0030 lr: 0.02\n",
            "iteration: 20450 loss: 0.0029 lr: 0.02\n",
            "iteration: 20460 loss: 0.0029 lr: 0.02\n",
            "iteration: 20470 loss: 0.0029 lr: 0.02\n",
            "iteration: 20480 loss: 0.0029 lr: 0.02\n",
            "iteration: 20490 loss: 0.0026 lr: 0.02\n",
            "iteration: 20500 loss: 0.0040 lr: 0.02\n",
            "iteration: 20510 loss: 0.0024 lr: 0.02\n",
            "iteration: 20520 loss: 0.0030 lr: 0.02\n",
            "iteration: 20530 loss: 0.0026 lr: 0.02\n",
            "iteration: 20540 loss: 0.0026 lr: 0.02\n",
            "iteration: 20550 loss: 0.0037 lr: 0.02\n",
            "iteration: 20560 loss: 0.0036 lr: 0.02\n",
            "iteration: 20570 loss: 0.0030 lr: 0.02\n",
            "iteration: 20580 loss: 0.0038 lr: 0.02\n",
            "iteration: 20590 loss: 0.0035 lr: 0.02\n",
            "iteration: 20600 loss: 0.0025 lr: 0.02\n",
            "iteration: 20610 loss: 0.0023 lr: 0.02\n",
            "iteration: 20620 loss: 0.0029 lr: 0.02\n",
            "iteration: 20630 loss: 0.0029 lr: 0.02\n",
            "iteration: 20640 loss: 0.0027 lr: 0.02\n",
            "iteration: 20650 loss: 0.0024 lr: 0.02\n",
            "iteration: 20660 loss: 0.0043 lr: 0.02\n",
            "iteration: 20670 loss: 0.0027 lr: 0.02\n",
            "iteration: 20680 loss: 0.0021 lr: 0.02\n",
            "iteration: 20690 loss: 0.0025 lr: 0.02\n",
            "iteration: 20700 loss: 0.0032 lr: 0.02\n",
            "iteration: 20710 loss: 0.0031 lr: 0.02\n",
            "iteration: 20720 loss: 0.0026 lr: 0.02\n",
            "iteration: 20730 loss: 0.0026 lr: 0.02\n",
            "iteration: 20740 loss: 0.0026 lr: 0.02\n",
            "iteration: 20750 loss: 0.0023 lr: 0.02\n",
            "iteration: 20760 loss: 0.0024 lr: 0.02\n",
            "iteration: 20770 loss: 0.0022 lr: 0.02\n",
            "iteration: 20780 loss: 0.0030 lr: 0.02\n",
            "iteration: 20790 loss: 0.0023 lr: 0.02\n",
            "iteration: 20800 loss: 0.0023 lr: 0.02\n",
            "iteration: 20810 loss: 0.0029 lr: 0.02\n",
            "iteration: 20820 loss: 0.0033 lr: 0.02\n",
            "iteration: 20830 loss: 0.0033 lr: 0.02\n",
            "iteration: 20840 loss: 0.0035 lr: 0.02\n",
            "iteration: 20850 loss: 0.0030 lr: 0.02\n",
            "iteration: 20860 loss: 0.0029 lr: 0.02\n",
            "iteration: 20870 loss: 0.0025 lr: 0.02\n",
            "iteration: 20880 loss: 0.0024 lr: 0.02\n",
            "iteration: 20890 loss: 0.0024 lr: 0.02\n",
            "iteration: 20900 loss: 0.0025 lr: 0.02\n",
            "iteration: 20910 loss: 0.0035 lr: 0.02\n",
            "iteration: 20920 loss: 0.0033 lr: 0.02\n",
            "iteration: 20930 loss: 0.0031 lr: 0.02\n",
            "iteration: 20940 loss: 0.0026 lr: 0.02\n",
            "iteration: 20950 loss: 0.0024 lr: 0.02\n",
            "iteration: 20960 loss: 0.0031 lr: 0.02\n",
            "iteration: 20970 loss: 0.0033 lr: 0.02\n",
            "iteration: 20980 loss: 0.0026 lr: 0.02\n",
            "iteration: 20990 loss: 0.0035 lr: 0.02\n",
            "iteration: 21000 loss: 0.0033 lr: 0.02\n",
            "iteration: 21010 loss: 0.0027 lr: 0.02\n",
            "iteration: 21020 loss: 0.0025 lr: 0.02\n",
            "iteration: 21030 loss: 0.0031 lr: 0.02\n",
            "iteration: 21040 loss: 0.0032 lr: 0.02\n",
            "iteration: 21050 loss: 0.0038 lr: 0.02\n",
            "iteration: 21060 loss: 0.0037 lr: 0.02\n",
            "iteration: 21070 loss: 0.0027 lr: 0.02\n",
            "iteration: 21080 loss: 0.0033 lr: 0.02\n",
            "iteration: 21090 loss: 0.0028 lr: 0.02\n",
            "iteration: 21100 loss: 0.0020 lr: 0.02\n",
            "iteration: 21110 loss: 0.0035 lr: 0.02\n",
            "iteration: 21120 loss: 0.0034 lr: 0.02\n",
            "iteration: 21130 loss: 0.0035 lr: 0.02\n",
            "iteration: 21140 loss: 0.0025 lr: 0.02\n",
            "iteration: 21150 loss: 0.0047 lr: 0.02\n",
            "iteration: 21160 loss: 0.0031 lr: 0.02\n",
            "iteration: 21170 loss: 0.0030 lr: 0.02\n",
            "iteration: 21180 loss: 0.0023 lr: 0.02\n",
            "iteration: 21190 loss: 0.0032 lr: 0.02\n",
            "iteration: 21200 loss: 0.0035 lr: 0.02\n",
            "iteration: 21210 loss: 0.0025 lr: 0.02\n",
            "iteration: 21220 loss: 0.0030 lr: 0.02\n",
            "iteration: 21230 loss: 0.0035 lr: 0.02\n",
            "iteration: 21240 loss: 0.0023 lr: 0.02\n",
            "iteration: 21250 loss: 0.0024 lr: 0.02\n",
            "iteration: 21260 loss: 0.0023 lr: 0.02\n",
            "iteration: 21270 loss: 0.0030 lr: 0.02\n",
            "iteration: 21280 loss: 0.0028 lr: 0.02\n",
            "iteration: 21290 loss: 0.0027 lr: 0.02\n",
            "iteration: 21300 loss: 0.0028 lr: 0.02\n",
            "iteration: 21310 loss: 0.0025 lr: 0.02\n",
            "iteration: 21320 loss: 0.0028 lr: 0.02\n",
            "iteration: 21330 loss: 0.0026 lr: 0.02\n",
            "iteration: 21340 loss: 0.0032 lr: 0.02\n",
            "iteration: 21350 loss: 0.0028 lr: 0.02\n",
            "iteration: 21360 loss: 0.0029 lr: 0.02\n",
            "iteration: 21370 loss: 0.0022 lr: 0.02\n",
            "iteration: 21380 loss: 0.0048 lr: 0.02\n",
            "iteration: 21390 loss: 0.0026 lr: 0.02\n",
            "iteration: 21400 loss: 0.0028 lr: 0.02\n",
            "iteration: 21410 loss: 0.0032 lr: 0.02\n",
            "iteration: 21420 loss: 0.0023 lr: 0.02\n",
            "iteration: 21430 loss: 0.0028 lr: 0.02\n",
            "iteration: 21440 loss: 0.0034 lr: 0.02\n",
            "iteration: 21450 loss: 0.0043 lr: 0.02\n",
            "iteration: 21460 loss: 0.0021 lr: 0.02\n",
            "iteration: 21470 loss: 0.0031 lr: 0.02\n",
            "iteration: 21480 loss: 0.0025 lr: 0.02\n",
            "iteration: 21490 loss: 0.0023 lr: 0.02\n",
            "iteration: 21500 loss: 0.0022 lr: 0.02\n",
            "iteration: 21510 loss: 0.0026 lr: 0.02\n",
            "iteration: 21520 loss: 0.0031 lr: 0.02\n",
            "iteration: 21530 loss: 0.0029 lr: 0.02\n",
            "iteration: 21540 loss: 0.0023 lr: 0.02\n",
            "iteration: 21550 loss: 0.0024 lr: 0.02\n",
            "iteration: 21560 loss: 0.0024 lr: 0.02\n",
            "iteration: 21570 loss: 0.0026 lr: 0.02\n",
            "iteration: 21580 loss: 0.0029 lr: 0.02\n",
            "iteration: 21590 loss: 0.0035 lr: 0.02\n",
            "iteration: 21600 loss: 0.0034 lr: 0.02\n",
            "iteration: 21610 loss: 0.0026 lr: 0.02\n",
            "iteration: 21620 loss: 0.0024 lr: 0.02\n",
            "iteration: 21630 loss: 0.0029 lr: 0.02\n",
            "iteration: 21640 loss: 0.0036 lr: 0.02\n",
            "iteration: 21650 loss: 0.0043 lr: 0.02\n",
            "iteration: 21660 loss: 0.0024 lr: 0.02\n",
            "iteration: 21670 loss: 0.0030 lr: 0.02\n",
            "iteration: 21680 loss: 0.0041 lr: 0.02\n",
            "iteration: 21690 loss: 0.0023 lr: 0.02\n",
            "iteration: 21700 loss: 0.0034 lr: 0.02\n",
            "iteration: 21710 loss: 0.0030 lr: 0.02\n",
            "iteration: 21720 loss: 0.0028 lr: 0.02\n",
            "iteration: 21730 loss: 0.0024 lr: 0.02\n",
            "iteration: 21740 loss: 0.0030 lr: 0.02\n",
            "iteration: 21750 loss: 0.0028 lr: 0.02\n",
            "iteration: 21760 loss: 0.0023 lr: 0.02\n",
            "iteration: 21770 loss: 0.0024 lr: 0.02\n",
            "iteration: 21780 loss: 0.0033 lr: 0.02\n",
            "iteration: 21790 loss: 0.0027 lr: 0.02\n",
            "iteration: 21800 loss: 0.0040 lr: 0.02\n",
            "iteration: 21810 loss: 0.0033 lr: 0.02\n",
            "iteration: 21820 loss: 0.0035 lr: 0.02\n",
            "iteration: 21830 loss: 0.0026 lr: 0.02\n",
            "iteration: 21840 loss: 0.0027 lr: 0.02\n",
            "iteration: 21850 loss: 0.0026 lr: 0.02\n",
            "iteration: 21860 loss: 0.0023 lr: 0.02\n",
            "iteration: 21870 loss: 0.0026 lr: 0.02\n",
            "iteration: 21880 loss: 0.0029 lr: 0.02\n",
            "iteration: 21890 loss: 0.0035 lr: 0.02\n",
            "iteration: 21900 loss: 0.0024 lr: 0.02\n",
            "iteration: 21910 loss: 0.0031 lr: 0.02\n",
            "iteration: 21920 loss: 0.0025 lr: 0.02\n",
            "iteration: 21930 loss: 0.0024 lr: 0.02\n",
            "iteration: 21940 loss: 0.0035 lr: 0.02\n",
            "iteration: 21950 loss: 0.0033 lr: 0.02\n",
            "iteration: 21960 loss: 0.0028 lr: 0.02\n",
            "iteration: 21970 loss: 0.0039 lr: 0.02\n",
            "iteration: 21980 loss: 0.0033 lr: 0.02\n",
            "iteration: 21990 loss: 0.0027 lr: 0.02\n",
            "iteration: 22000 loss: 0.0031 lr: 0.02\n",
            "iteration: 22010 loss: 0.0029 lr: 0.02\n",
            "iteration: 22020 loss: 0.0032 lr: 0.02\n",
            "iteration: 22030 loss: 0.0031 lr: 0.02\n",
            "iteration: 22040 loss: 0.0041 lr: 0.02\n",
            "iteration: 22050 loss: 0.0028 lr: 0.02\n",
            "iteration: 22060 loss: 0.0040 lr: 0.02\n",
            "iteration: 22070 loss: 0.0038 lr: 0.02\n",
            "iteration: 22080 loss: 0.0027 lr: 0.02\n",
            "iteration: 22090 loss: 0.0025 lr: 0.02\n",
            "iteration: 22100 loss: 0.0025 lr: 0.02\n",
            "iteration: 22110 loss: 0.0023 lr: 0.02\n",
            "iteration: 22120 loss: 0.0029 lr: 0.02\n",
            "iteration: 22130 loss: 0.0028 lr: 0.02\n",
            "iteration: 22140 loss: 0.0026 lr: 0.02\n",
            "iteration: 22150 loss: 0.0032 lr: 0.02\n",
            "iteration: 22160 loss: 0.0045 lr: 0.02\n",
            "iteration: 22170 loss: 0.0026 lr: 0.02\n",
            "iteration: 22180 loss: 0.0033 lr: 0.02\n",
            "iteration: 22190 loss: 0.0025 lr: 0.02\n",
            "iteration: 22200 loss: 0.0031 lr: 0.02\n",
            "iteration: 22210 loss: 0.0025 lr: 0.02\n",
            "iteration: 22220 loss: 0.0024 lr: 0.02\n",
            "iteration: 22230 loss: 0.0038 lr: 0.02\n",
            "iteration: 22240 loss: 0.0027 lr: 0.02\n",
            "iteration: 22250 loss: 0.0025 lr: 0.02\n",
            "iteration: 22260 loss: 0.0028 lr: 0.02\n",
            "iteration: 22270 loss: 0.0028 lr: 0.02\n",
            "iteration: 22280 loss: 0.0030 lr: 0.02\n",
            "iteration: 22290 loss: 0.0026 lr: 0.02\n",
            "iteration: 22300 loss: 0.0028 lr: 0.02\n",
            "iteration: 22310 loss: 0.0030 lr: 0.02\n",
            "iteration: 22320 loss: 0.0029 lr: 0.02\n",
            "iteration: 22330 loss: 0.0026 lr: 0.02\n",
            "iteration: 22340 loss: 0.0026 lr: 0.02\n",
            "iteration: 22350 loss: 0.0025 lr: 0.02\n",
            "iteration: 22360 loss: 0.0024 lr: 0.02\n",
            "iteration: 22370 loss: 0.0033 lr: 0.02\n",
            "iteration: 22380 loss: 0.0023 lr: 0.02\n",
            "iteration: 22390 loss: 0.0030 lr: 0.02\n",
            "iteration: 22400 loss: 0.0024 lr: 0.02\n",
            "iteration: 22410 loss: 0.0026 lr: 0.02\n",
            "iteration: 22420 loss: 0.0022 lr: 0.02\n",
            "iteration: 22430 loss: 0.0023 lr: 0.02\n",
            "iteration: 22440 loss: 0.0027 lr: 0.02\n",
            "iteration: 22450 loss: 0.0033 lr: 0.02\n",
            "iteration: 22460 loss: 0.0032 lr: 0.02\n",
            "iteration: 22470 loss: 0.0021 lr: 0.02\n",
            "iteration: 22480 loss: 0.0027 lr: 0.02\n",
            "iteration: 22490 loss: 0.0025 lr: 0.02\n",
            "iteration: 22500 loss: 0.0022 lr: 0.02\n",
            "iteration: 22510 loss: 0.0021 lr: 0.02\n",
            "iteration: 22520 loss: 0.0029 lr: 0.02\n",
            "iteration: 22530 loss: 0.0025 lr: 0.02\n",
            "iteration: 22540 loss: 0.0028 lr: 0.02\n",
            "iteration: 22550 loss: 0.0025 lr: 0.02\n",
            "iteration: 22560 loss: 0.0037 lr: 0.02\n",
            "iteration: 22570 loss: 0.0026 lr: 0.02\n",
            "iteration: 22580 loss: 0.0028 lr: 0.02\n",
            "iteration: 22590 loss: 0.0044 lr: 0.02\n",
            "iteration: 22600 loss: 0.0047 lr: 0.02\n",
            "iteration: 22610 loss: 0.0031 lr: 0.02\n",
            "iteration: 22620 loss: 0.0026 lr: 0.02\n",
            "iteration: 22630 loss: 0.0025 lr: 0.02\n",
            "iteration: 22640 loss: 0.0024 lr: 0.02\n",
            "iteration: 22650 loss: 0.0027 lr: 0.02\n",
            "iteration: 22660 loss: 0.0036 lr: 0.02\n",
            "iteration: 22670 loss: 0.0035 lr: 0.02\n",
            "iteration: 22680 loss: 0.0024 lr: 0.02\n",
            "iteration: 22690 loss: 0.0036 lr: 0.02\n",
            "iteration: 22700 loss: 0.0029 lr: 0.02\n",
            "iteration: 22710 loss: 0.0031 lr: 0.02\n",
            "iteration: 22720 loss: 0.0034 lr: 0.02\n",
            "iteration: 22730 loss: 0.0041 lr: 0.02\n",
            "iteration: 22740 loss: 0.0028 lr: 0.02\n",
            "iteration: 22750 loss: 0.0035 lr: 0.02\n",
            "iteration: 22760 loss: 0.0029 lr: 0.02\n",
            "iteration: 22770 loss: 0.0025 lr: 0.02\n",
            "iteration: 22780 loss: 0.0024 lr: 0.02\n",
            "iteration: 22790 loss: 0.0030 lr: 0.02\n",
            "iteration: 22800 loss: 0.0023 lr: 0.02\n",
            "iteration: 22810 loss: 0.0026 lr: 0.02\n",
            "iteration: 22820 loss: 0.0024 lr: 0.02\n",
            "iteration: 22830 loss: 0.0033 lr: 0.02\n",
            "iteration: 22840 loss: 0.0035 lr: 0.02\n",
            "iteration: 22850 loss: 0.0036 lr: 0.02\n",
            "iteration: 22860 loss: 0.0032 lr: 0.02\n",
            "iteration: 22870 loss: 0.0025 lr: 0.02\n",
            "iteration: 22880 loss: 0.0037 lr: 0.02\n",
            "iteration: 22890 loss: 0.0030 lr: 0.02\n",
            "iteration: 22900 loss: 0.0027 lr: 0.02\n",
            "iteration: 22910 loss: 0.0026 lr: 0.02\n",
            "iteration: 22920 loss: 0.0027 lr: 0.02\n",
            "iteration: 22930 loss: 0.0027 lr: 0.02\n",
            "iteration: 22940 loss: 0.0052 lr: 0.02\n",
            "iteration: 22950 loss: 0.0028 lr: 0.02\n",
            "iteration: 22960 loss: 0.0026 lr: 0.02\n",
            "iteration: 22970 loss: 0.0026 lr: 0.02\n",
            "iteration: 22980 loss: 0.0027 lr: 0.02\n",
            "iteration: 22990 loss: 0.0035 lr: 0.02\n",
            "iteration: 23000 loss: 0.0032 lr: 0.02\n",
            "iteration: 23010 loss: 0.0033 lr: 0.02\n",
            "iteration: 23020 loss: 0.0034 lr: 0.02\n",
            "iteration: 23030 loss: 0.0028 lr: 0.02\n",
            "iteration: 23040 loss: 0.0027 lr: 0.02\n",
            "iteration: 23050 loss: 0.0022 lr: 0.02\n",
            "iteration: 23060 loss: 0.0026 lr: 0.02\n",
            "iteration: 23070 loss: 0.0036 lr: 0.02\n",
            "iteration: 23080 loss: 0.0034 lr: 0.02\n",
            "iteration: 23090 loss: 0.0032 lr: 0.02\n",
            "iteration: 23100 loss: 0.0027 lr: 0.02\n",
            "iteration: 23110 loss: 0.0025 lr: 0.02\n",
            "iteration: 23120 loss: 0.0027 lr: 0.02\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-3171b8adcb1e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdlc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_config_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSHUF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplayiters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msaveiters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/deeplabcut/pose_estimation_tensorflow/training.py\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(config, shuffle, trainingsetindex, max_snapshots_to_keep, displayiters, saveiters, maxiters, allow_growth, gputouse, autotune, keepdeconvweights, modelprefix)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/deeplabcut/pose_estimation_tensorflow/training.py\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(config, shuffle, trainingsetindex, max_snapshots_to_keep, displayiters, saveiters, maxiters, allow_growth, gputouse, autotune, keepdeconvweights, modelprefix)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Selecting single-animal trainer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m             train(\n\u001b[0m\u001b[1;32m    213\u001b[0m                 \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposeconfigfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0mdisplayiters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/deeplabcut/pose_estimation_tensorflow/core/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(config_yaml, displayiters, saveiters, maxiters, max_to_keep, keepdeconvweights, allow_growth)\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0mlr_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcurrent_lr\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         [_, loss_val, summary] = sess.run(\n\u001b[0m\u001b[1;32m    284\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged_summaries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[1;32m    969\u001b[0m                          run_metadata_ptr)\n\u001b[1;32m    970\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;31m# or if the call is a partial run that specifies feeds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0m\u001b[1;32m   1192\u001b[0m                              feed_dict_tensor, options, run_metadata)\n\u001b[1;32m   1193\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0m\u001b[1;32m   1372\u001b[0m                            run_metadata)\n\u001b[1;32m   1373\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1376\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1378\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1379\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1359\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0m\u001b[1;32m   1362\u001b[0m                                       target_list, run_metadata)\n\u001b[1;32m   1363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1452\u001b[0m   def _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list,\n\u001b[1;32m   1453\u001b[0m                           run_metadata):\n\u001b[0;32m-> 1454\u001b[0;31m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0m\u001b[1;32m   1455\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m                                             run_metadata)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dlc.evaluate_network(path_config_file,Shuffles=[SHUF], plotting=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--Gq5BRc2UIr",
        "outputId": "eee6913a-9015-4539-c96e-144a1b7501a8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Config:\n",
            "{'all_joints': [[0], [1], [2], [3]],\n",
            " 'all_joints_names': ['left eye', 'right eye', 'lower head', 'tail'],\n",
            " 'batch_size': 1,\n",
            " 'crop_pad': 0,\n",
            " 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_testJul6/test_dlc95shuffle1.mat',\n",
            " 'dataset_type': 'imgaug',\n",
            " 'deterministic': False,\n",
            " 'fg_fraction': 0.25,\n",
            " 'global_scale': 0.8,\n",
            " 'init_weights': '/usr/local/lib/python3.10/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt',\n",
            " 'intermediate_supervision': False,\n",
            " 'intermediate_supervision_layer': 12,\n",
            " 'location_refinement': True,\n",
            " 'locref_huber_loss': True,\n",
            " 'locref_loss_weight': 1.0,\n",
            " 'locref_stdev': 7.2801,\n",
            " 'log_dir': 'log',\n",
            " 'mean_pixel': [123.68, 116.779, 103.939],\n",
            " 'mirror': False,\n",
            " 'net_type': 'resnet_50',\n",
            " 'num_joints': 4,\n",
            " 'optimizer': 'sgd',\n",
            " 'pairwise_huber_loss': True,\n",
            " 'pairwise_predict': False,\n",
            " 'partaffinityfield_predict': False,\n",
            " 'regularize': False,\n",
            " 'scoremap_dir': 'test',\n",
            " 'shuffle': True,\n",
            " 'snapshot_prefix': '/content/drive/My '\n",
            "                    'Drive/test-dlc-2023-07-06/dlc-models/iteration-0/testJul6-trainset95shuffle1/test/snapshot',\n",
            " 'stride': 8.0,\n",
            " 'weigh_negatives': False,\n",
            " 'weigh_only_present_joints': False,\n",
            " 'weigh_part_predictions': False,\n",
            " 'weight_decay': 0.0001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running  DLC_resnet50_testJul6shuffle1_23000  with # of training iterations: 23000\n",
            "This net has already been evaluated!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dlc.analyze_videos(path_config_file, videofile_path, videotype=VideoType, shuffle=SHUF)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 923
        },
        "id": "ms_imJU19KkW",
        "outputId": "c4f4e6be-42a2-4f84-ca44-81d05da3906b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Config:\n",
            "{'all_joints': [[0], [1], [2], [3]],\n",
            " 'all_joints_names': ['left eye', 'right eye', 'lower head', 'tail'],\n",
            " 'batch_size': 1,\n",
            " 'crop_pad': 0,\n",
            " 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_testJul6/test_dlc95shuffle1.mat',\n",
            " 'dataset_type': 'imgaug',\n",
            " 'deterministic': False,\n",
            " 'fg_fraction': 0.25,\n",
            " 'global_scale': 0.8,\n",
            " 'init_weights': '/usr/local/lib/python3.10/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt',\n",
            " 'intermediate_supervision': False,\n",
            " 'intermediate_supervision_layer': 12,\n",
            " 'location_refinement': True,\n",
            " 'locref_huber_loss': True,\n",
            " 'locref_loss_weight': 1.0,\n",
            " 'locref_stdev': 7.2801,\n",
            " 'log_dir': 'log',\n",
            " 'mean_pixel': [123.68, 116.779, 103.939],\n",
            " 'mirror': False,\n",
            " 'net_type': 'resnet_50',\n",
            " 'num_joints': 4,\n",
            " 'optimizer': 'sgd',\n",
            " 'pairwise_huber_loss': True,\n",
            " 'pairwise_predict': False,\n",
            " 'partaffinityfield_predict': False,\n",
            " 'regularize': False,\n",
            " 'scoremap_dir': 'test',\n",
            " 'shuffle': True,\n",
            " 'snapshot_prefix': '/content/drive/My '\n",
            "                    'Drive/test-dlc-2023-07-06/dlc-models/iteration-0/testJul6-trainset95shuffle1/test/snapshot',\n",
            " 'stride': 8.0,\n",
            " 'weigh_negatives': False,\n",
            " 'weigh_only_present_joints': False,\n",
            " 'weigh_part_predictions': False,\n",
            " 'weight_decay': 0.0001}\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  warnings.warn('`layer.apply` is deprecated and '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using snapshot-23000 for model /content/drive/My Drive/test-dlc-2023-07-06/dlc-models/iteration-0/testJul6-trainset95shuffle1\n",
            "Analyzing all the videos in the directory...\n",
            "Starting to analyze %  /content/drive/My Drive/test-dlc-2023-07-06/videos/temp-06272023104645-0000.avi\n",
            "Loading  /content/drive/My Drive/test-dlc-2023-07-06/videos/temp-06272023104645-0000.avi\n",
            "Duration of video [s]:  11.04 , recorded with  149.91 fps!\n",
            "Overall # of frames:  1655  found with (before cropping) frame dimensions:  916 998\n",
            "Starting to extract posture\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1655/1655 [02:12<00:00, 12.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving results in /content/drive/My Drive/test-dlc-2023-07-06/videos...\n",
            "The videos are analyzed. Now your research can truly start! \n",
            " You can create labeled videos with 'create_labeled_video'\n",
            "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'DLC_resnet50_testJul6shuffle1_23000'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dlc.filterpredictions(path_config_file, videofile_path, videotype=VideoType, shuffle=SHUF)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNz65iQPBgKy",
        "outputId": "343040cb-7d8f-4938-879b-2900ff201959"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing all the videos in the directory...\n",
            "Filtering with median model /content/drive/My Drive/test-dlc-2023-07-06/videos/temp-06272023104645-0000.avi\n",
            "Saving filtered csv poses!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dlc.create_labeled_video(path_config_file,videofile_path, videotype=VideoType, shuffle=SHUF, filtered=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWFFXy-aCJTe",
        "outputId": "581e2a32-66bf-4ca9-9fe1-8c6f21f6d4e2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing all the videos in the directory...\n",
            "Starting to process video: /content/drive/My Drive/test-dlc-2023-07-06/videos/temp-06272023104645-0000.avi\n",
            "Loading /content/drive/My Drive/test-dlc-2023-07-06/videos/temp-06272023104645-0000.avi and data.\n",
            "Duration of video [s]: 11.03, recorded with 150 fps!\n",
            "Overall # of frames: 1655 with cropped frame dimensions: 916 998\n",
            "Generating frames and creating video.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1655/1655 [00:24<00:00, 67.65it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[True]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}